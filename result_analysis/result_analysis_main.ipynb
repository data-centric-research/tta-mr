{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overal Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入库\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 添加项目根目录到 sys.path\n",
    "notebook_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, \"..\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "cotta_root = os.path.join(project_root, 'baseline_code/cotta-main/cifar')\n",
    "sys.path.append(cotta_root)\n",
    "\n",
    "plf_root = os.path.join(project_root, 'baseline_code/PLF-main/cifar')\n",
    "sys.path.append(plf_root)\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "\n",
    "from core_model.custom_model import ClassifierWrapper, load_custom_model\n",
    "from core_model.dataset import get_dataset_loader\n",
    "\n",
    "from core_model.train_test import model_test\n",
    "\n",
    "from configs import settings\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import cotta\n",
    "import plf\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from cfgs.conf_cotta import cfg as cfg_cotta\n",
    "from cfgs.conf_plf import cfg as cfg_plf\n",
    "\n",
    "\n",
    "from args_paser import parse_args\n",
    "\n",
    "\n",
    "# 设置设备\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# sys.argv = ['', '--dataset','cifar-10', '--model', 'cifar-resnet18']\n",
    "# custom_args = parse_args()\n",
    "# custom_args\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 通用函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_cotta(model, args):\n",
    "    \"\"\"Set up tent adaptation.\n",
    "\n",
    "    Configure the model for training + feature modulation by batch statistics,\n",
    "    collect the parameters for feature modulation by gradient optimization,\n",
    "    set up the optimizer, and then tent the model.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def setup_optimizer(params):\n",
    "        \"\"\"Set up optimizer for tent adaptation.\n",
    "\n",
    "        Tent needs an optimizer for test-time entropy minimization.\n",
    "        In principle, tent could make use of any gradient optimizer.\n",
    "        In practice, we advise choosing Adam or SGD+momentum.\n",
    "        For optimization settings, we advise to use the settings from the end of\n",
    "        trainig, if known, or start with a low learning rate (like 0.001) if not.\n",
    "\n",
    "        For best results, try tuning the learning rate and batch size.\n",
    "        \"\"\"\n",
    "        if cfg_cotta.OPTIM.METHOD == \"Adam\":\n",
    "            return optim.Adam(\n",
    "                params,\n",
    "                lr=cfg_cotta.OPTIM.LR,\n",
    "                betas=(cfg_cotta.OPTIM.BETA, 0.999),\n",
    "                weight_decay=cfg_cotta.OPTIM.WD,\n",
    "            )\n",
    "        elif cfg_cotta.OPTIM.METHOD == \"SGD\":\n",
    "            return optim.SGD(\n",
    "                params,\n",
    "                lr=cfg_cotta.OPTIM.LR,\n",
    "                momentum=cfg_cotta.OPTIM.MOMENTUM,\n",
    "                dampening=cfg_cotta.OPTIM.DAMPENING,\n",
    "                weight_decay=cfg_cotta.OPTIM.WD,\n",
    "                nesterov=cfg_cotta.OPTIM.NESTEROV,\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    model = cotta.configure_model(model)\n",
    "    params, param_names = cotta.collect_params(model)\n",
    "    optimizer = setup_optimizer(params)\n",
    "    cotta_model = cotta.CoTTA(\n",
    "        model,\n",
    "        optimizer,\n",
    "        args,\n",
    "        steps=cfg_cotta.OPTIM.STEPS,\n",
    "        episodic=cfg_cotta.MODEL.EPISODIC,\n",
    "        mt_alpha=cfg_cotta.OPTIM.MT,\n",
    "        rst_m=cfg_cotta.OPTIM.RST,\n",
    "        ap=cfg_cotta.OPTIM.AP,\n",
    "    )\n",
    "    return cotta_model\n",
    "\n",
    "\n",
    "\n",
    "def setup_plf(model, custom_args, num_classes):\n",
    "    \"\"\"Set up tent adaptation.\n",
    "\n",
    "    Configure the model for training + feature modulation by batch statistics,\n",
    "    collect the parameters for feature modulation by gradient optimization,\n",
    "    set up the optimizer, and then tent the model.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "    def setup_optimizer(params):\n",
    "        \"\"\"Set up optimizer for tent adaptation.\n",
    "\n",
    "        Tent needs an optimizer for test-time entropy minimization.\n",
    "        In principle, tent could make use of any gradient optimizer.\n",
    "        In practice, we advise choosing Adam or SGD+momentum.\n",
    "        For optimization settings, we advise to use the settings from the end of\n",
    "        trainig, if known, or start with a low learning rate (like 0.001) if not.\n",
    "\n",
    "        For best results, try tuning the learning rate and batch size.\n",
    "        \"\"\"\n",
    "        if cfg_plf.OPTIM.METHOD == \"Adam\":\n",
    "            return optim.Adam(\n",
    "                params,\n",
    "                lr=cfg_plf.OPTIM.LR,\n",
    "                betas=(cfg_plf.OPTIM.BETA, 0.999),\n",
    "                weight_decay=cfg_plf.OPTIM.WD,\n",
    "            )\n",
    "        elif cfg_plf.OPTIM.METHOD == \"SGD\":\n",
    "            return optim.SGD(\n",
    "                params,\n",
    "                lr=cfg_plf.OPTIM.LR,\n",
    "                momentum=cfg_plf.OPTIM.MOMENTUM,\n",
    "                dampening=cfg_plf.OPTIM.DAMPENING,\n",
    "                weight_decay=cfg_plf.OPTIM.WD,\n",
    "                nesterov=cfg_plf.OPTIM.NESTEROV,\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "\n",
    "    model = plf.configure_model(model)\n",
    "    params, param_names = plf.collect_params(model)\n",
    "    optimizer = setup_optimizer(params)\n",
    "    plf_model = plf.PLF(\n",
    "        model,\n",
    "        optimizer,\n",
    "        custom_args,\n",
    "        steps=cfg_plf.OPTIM.STEPS,\n",
    "        episodic=cfg_plf.MODEL.EPISODIC,\n",
    "        mt_alpha=cfg_plf.OPTIM.MT,\n",
    "        rst_m=cfg_plf.OPTIM.RST,\n",
    "        ap=cfg_plf.OPTIM.AP,\n",
    "        num_classes=num_classes,\n",
    "    )\n",
    "    return plf_model\n",
    "\n",
    "\n",
    "def clean_accuracy(model: nn.Module,\n",
    "                   x: torch.Tensor,\n",
    "                   y: torch.Tensor,\n",
    "                   batch_size: int = 100,\n",
    "                   device: torch.device = None):\n",
    "    if device is None:\n",
    "        device = x.device\n",
    "    acc = 0.\n",
    "    n_batches = math.ceil(x.shape[0] / batch_size)\n",
    "    with torch.no_grad():\n",
    "        for counter in trange(n_batches):\n",
    "            x_curr = x[counter * batch_size:(counter + 1) *\n",
    "                       batch_size].to(device)\n",
    "            y_curr = y[counter * batch_size:(counter + 1) *\n",
    "                       batch_size].to(device)\n",
    "\n",
    "            # print(f'Batch with sample num: {len(x_curr)}')\n",
    "            output = model(x_curr)\n",
    "            corrected_num = (output.max(1)[1] == y_curr).float().sum()\n",
    "            acc += corrected_num\n",
    "            \n",
    "            # [2024-10-10 sunzekun] 屏蔽了结果输出，保持界面整洁\n",
    "            # print('batch %d, corrected_num: %d' % (counter, corrected_num.item()))\n",
    "        # save step model_tta\n",
    "    return acc.item() / x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_acc(test_loader, model, device):\n",
    "\n",
    "    # Run-Experiment代码里的评估代码。\n",
    "    # 只能测试总体的test_acc\n",
    "    # 放在这里只是为了检查一下错误是不是发生在eva中。实际上可能不用。\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.eval().to(device)   \n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(\n",
    "            total=len(test_loader), desc=f\"Testing\"\n",
    "        ) as pbar:\n",
    "            for test_inputs, test_targets in test_loader:\n",
    "                test_inputs, test_targets = test_inputs.to(device), test_targets.to(\n",
    "                    device\n",
    "                )\n",
    "                test_outputs = model(test_inputs)\n",
    "                loss = criterion(test_outputs, test_targets)\n",
    "                _, predicted_test = torch.max(test_outputs, 1)\n",
    "                total_test += test_targets.size(0)\n",
    "                correct_test += (predicted_test == test_targets).sum().item()\n",
    "\n",
    "                # 更新进度条\n",
    "                pbar.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "                pbar.update(1)\n",
    "\n",
    "    test_accuracy = 100 * correct_test / total_test\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    return test_accuracy\n",
    "        \n",
    "\n",
    "\n",
    "class BaseTensorDataset(Dataset):\n",
    "\n",
    "    # Run-Experiment代码里的自定义数据集。\n",
    "    # 放在这里只是为了检查一下错误是不是发生在数据集中。实际上可能不用。\n",
    "\n",
    "    def __init__(self, data, labels, transforms=None, device=None):\n",
    "        self.data = torch.as_tensor(data, device=device)\n",
    "        self.labels = torch.as_tensor(labels, device=device)\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        if self.transforms is not None:\n",
    "            self.transforms(data)\n",
    "\n",
    "        return data, self.labels[index]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_suffix(method, step):\n",
    "    # 不同的method有不同的后缀\n",
    "    # 例如，contra有restore和tta的，我们在综合测评中应该只考虑tta\n",
    "    # 并且根据step还有所不同。step0的情况下，所有的suffix都为worker_restore.\n",
    "    # 因此，需要一个函数专门处理各种情况\n",
    "    \n",
    "    # Step-0\n",
    "    if step == 0:\n",
    "        return \"worker_restore\"\n",
    "    \n",
    "    # Step-1,2,3\n",
    "    if method in ['cotta', 'plf', 'contra']:\n",
    "        return \"worker_tta\"\n",
    "    else:\n",
    "        return \"worker_restore\"\n",
    "    \n",
    "def eva_test_acc(dataset_name, model_name, noise_type='symmetric'):\n",
    "    \"\"\"\n",
    "    核心代码，用来评估指定dataset任务下的所有方法的step0~4的测试准确率\n",
    "    具体测哪些组可以在methods这里更换\n",
    "    \"\"\"\n",
    "    \n",
    "    case = settings.get_case(noise_ratio=0.2, noise_type=noise_type, balanced=True)\n",
    "    mean, std = None, None\n",
    "    num_classes = settings.num_classes_dict[dataset_name]\n",
    "    print(f'目前测试的数据集：{dataset_name}, case模式：{case}')\n",
    "\n",
    "    # 读入测试数据集    \n",
    "    # core.py中使用的数据集读入代码\n",
    "    test_data, test_labels, test_dataloader = get_dataset_loader(\n",
    "        dataset_name, \"test\", case, None, mean, std, batch_size=128, shuffle=False\n",
    "    )\n",
    "\n",
    "    # run_experiment.py中的代码, 用于对比验证效果。\n",
    "    # print(f'Targeted dataset: {settings.get_dataset_path(dataset_name, case, \"test_data\")}')\n",
    "\n",
    "    # D_test_data = np.load(\n",
    "    #     settings.get_dataset_path(dataset_name, case, \"test_data\")\n",
    "    # )\n",
    "    # D_test_labels = np.load(\n",
    "    #     settings.get_dataset_path(dataset_name, case, \"test_label\")\n",
    "    # )\n",
    "    # test_dataset = BaseTensorDataset(D_test_data, D_test_labels, device=device)\n",
    "    # test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "    # [Debug用]-检查一下数据\n",
    "    # x, y = next(iter(test_dataloader))\n",
    "    # print(x[0])\n",
    "        \n",
    "\n",
    "    # steps = [f'step_{i}' for i in range(4)]\n",
    "    # methods = ['contra']\n",
    "    # methods = ['raw']\n",
    "    # methods = ['raw', 'coteaching', 'coteaching_plus', 'jocor', 'cotta', 'plf', 'contra']\n",
    "    \n",
    "    # methods = ['cotta']\n",
    "    # methods = ['plf']\n",
    "    methods = ['raw', 'coteaching', 'coteaching_plus', 'jocor', 'contra']\n",
    "    steps = [i for i in range(4)]\n",
    "\n",
    "    df = pd.DataFrame(index=methods, columns=steps)\n",
    "\n",
    "\n",
    "    # for method in tqdm(methods):\n",
    "    \n",
    "    for method in methods:\n",
    "        for step in steps:\n",
    "            # 先确定一下模型的suffix\n",
    "            model_suffix = get_suffix(method, step)\n",
    "            # 读入模型架构\n",
    "            # 放到循环里边，每次都要重新新建一个模型。\n",
    "            # 牺牲了一些速度，但是好处是防止cotta和plf把模型架构更改了，导致repair类的模型出问题\n",
    "            model = load_custom_model(model_name, num_classes, load_pretrained=False)\n",
    "            model = ClassifierWrapper(model, num_classes)\n",
    "\n",
    "            # 按照模型名和step数读入模型参数\n",
    "            model_repair_save_path = settings.get_ckpt_path(dataset_name, case, model_name, model_suffix=model_suffix, step=step, unique_name=method)\n",
    "            print(f'Evaluating {model_repair_save_path}')\n",
    "\n",
    "            # checkpoint = torch.load(model_repair_save_path)\n",
    "            try:\n",
    "                checkpoint = torch.load(model_repair_save_path)\n",
    "            except:\n",
    "                print(f\"Cannot find the weight file at {model_repair_save_path}. Just SKIP.\")\n",
    "                continue\n",
    "            model.load_state_dict(checkpoint, strict=False)\n",
    "\n",
    "\n",
    "            # [24-10-10 sunzekun] 有两个特殊的tta模型：cotta和plf，不仅改了参数，而且使用了特定的模型架构来进行推断。\n",
    "            # (即，不仅有test-time Adaptation，还有Augmentation)\n",
    "            # 所以对这两种需要另外写代码实现。\n",
    "            # 注意，contra没有这个特殊过程。\n",
    "            if method == 'cotta' or method == 'plf':\n",
    "                # 由于测试代码的jupyternotebook 构建命令行参数很麻烦\n",
    "                # 这里暂时去掉了dataset，model这两个必选参数的required = True\n",
    "                # 而是直接在使用的时候复制。\n",
    "                sys.argv = ['', '--dataset', dataset_name, '--model', model_name, '--uni_name', method, '--balanced']\n",
    "                custom_args = parse_args()\n",
    "                model.eval().to(device)\n",
    "                if method == 'cotta':\n",
    "                    model_aug = setup_cotta(model, custom_args)\n",
    "                else:\n",
    "                    model_aug = setup_plf(model, custom_args, num_classes)\n",
    "                \n",
    "                try:\n",
    "                    model_aug.reset()\n",
    "                except:\n",
    "                    print(f'Failed to reset')\n",
    "                \n",
    "                x_test = torch.from_numpy(test_data)\n",
    "                y_test = torch.from_numpy(test_labels)\n",
    "                x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "\n",
    "                test_acc = clean_accuracy(model_aug, x_test, y_test, batch_size=1024, device=device)                \n",
    "                # print(test_acc)\n",
    "            else:\n",
    "                test_acc = model_test(test_dataloader, model, device=device)\n",
    "                # print(test_acc)\n",
    "                test_acc = test_acc['global']\n",
    "            \n",
    "            print(f\"测试集Acc：{test_acc}\")\n",
    "            df.loc[method, step] = test_acc\n",
    "            # test_acc = get_test_acc(test_dataloader, model, device=device)\n",
    "            # df.loc[method, step] = test_acc\n",
    "\n",
    "    results_dir = './results_main'\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # 保留参考代码，万一想用excel保存结果的时候用下面的\n",
    "    # results_file_name = f'{dataset_name}.xlsx'\n",
    "    # results_file_dir = os.path.join(results_dir, results_file_name)\n",
    "    # with pd.ExcelWriter(results_file_dir, engine='openpyxl') as writer:\n",
    "    #     df.to_excel(writer)\n",
    "        # for tab, df in dfs.items():\n",
    "\n",
    "    # cls: 分类任务\n",
    "    # rtv: 检索任务\n",
    "    mission_type = 'cls' if noise_type == 'symmetric' else 'rtv'\n",
    "    results_file_name = f'{dataset_name}_{mission_type}.csv'\n",
    "    results_file_dir = os.path.join(results_dir, results_file_name)\n",
    "    df.to_csv(results_file_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cifar-10 分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目前测试的数据集：cifar-10, case模式：nr_0.2_nt_symmetric_balanced\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_0/raw/cifar-resnet18_worker_restore.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_59599/1549805960.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_repair_save_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 72.19\n",
      "label: 0, acc: 75.30\n",
      "label: 1, acc: 89.80\n",
      "label: 2, acc: 58.90\n",
      "label: 3, acc: 42.60\n",
      "label: 4, acc: 65.60\n",
      "label: 5, acc: 64.50\n",
      "label: 6, acc: 81.30\n",
      "label: 7, acc: 75.30\n",
      "label: 8, acc: 87.30\n",
      "label: 9, acc: 81.30\n",
      "测试集Acc：0.7219\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_1/raw/cifar-resnet18_worker_restore.pth\n",
      "test_acc: 76.56\n",
      "label: 0, acc: 82.20\n",
      "label: 1, acc: 91.80\n",
      "label: 2, acc: 65.90\n",
      "label: 3, acc: 48.00\n",
      "label: 4, acc: 71.90\n",
      "label: 5, acc: 69.40\n",
      "label: 6, acc: 87.30\n",
      "label: 7, acc: 80.60\n",
      "label: 8, acc: 85.50\n",
      "label: 9, acc: 83.00\n",
      "测试集Acc：0.7656\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_2/raw/cifar-resnet18_worker_restore.pth\n",
      "test_acc: 74.12\n",
      "label: 0, acc: 80.80\n",
      "label: 1, acc: 91.40\n",
      "label: 2, acc: 67.50\n",
      "label: 3, acc: 43.70\n",
      "label: 4, acc: 70.60\n",
      "label: 5, acc: 62.10\n",
      "label: 6, acc: 84.00\n",
      "label: 7, acc: 76.00\n",
      "label: 8, acc: 84.20\n",
      "label: 9, acc: 80.90\n",
      "测试集Acc：0.7412\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_3/raw/cifar-resnet18_worker_restore.pth\n",
      "test_acc: 68.91\n",
      "label: 0, acc: 81.30\n",
      "label: 1, acc: 85.30\n",
      "label: 2, acc: 66.60\n",
      "label: 3, acc: 26.00\n",
      "label: 4, acc: 73.50\n",
      "label: 5, acc: 53.20\n",
      "label: 6, acc: 84.80\n",
      "label: 7, acc: 58.20\n",
      "label: 8, acc: 87.80\n",
      "label: 9, acc: 72.40\n",
      "测试集Acc：0.6891\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_0/coteaching/cifar-resnet18_worker_restore.pth\n",
      "test_acc: 27.19\n",
      "label: 0, acc: 0.00\n",
      "label: 1, acc: 35.10\n",
      "label: 2, acc: 2.40\n",
      "label: 3, acc: 0.30\n",
      "label: 4, acc: 23.30\n",
      "label: 5, acc: 6.80\n",
      "label: 6, acc: 60.90\n",
      "label: 7, acc: 38.00\n",
      "label: 8, acc: 75.90\n",
      "label: 9, acc: 29.20\n",
      "测试集Acc：0.2719\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_1/coteaching/cifar-resnet18_worker_restore.pth\n",
      "test_acc: 11.42\n",
      "label: 0, acc: 0.00\n",
      "label: 1, acc: 35.60\n",
      "label: 2, acc: 0.00\n",
      "label: 3, acc: 0.00\n",
      "label: 4, acc: 0.00\n",
      "label: 5, acc: 0.00\n",
      "label: 6, acc: 0.00\n",
      "label: 7, acc: 78.60\n",
      "label: 8, acc: 0.00\n",
      "label: 9, acc: 0.00\n",
      "测试集Acc：0.1142\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_2/coteaching/cifar-resnet18_worker_restore.pth\n",
      "test_acc: 10.01\n",
      "label: 0, acc: 0.00\n",
      "label: 1, acc: 64.40\n",
      "label: 2, acc: 0.00\n",
      "label: 3, acc: 0.00\n",
      "label: 4, acc: 0.00\n",
      "label: 5, acc: 0.00\n",
      "label: 6, acc: 0.00\n",
      "label: 7, acc: 35.70\n",
      "label: 8, acc: 0.00\n",
      "label: 9, acc: 0.00\n",
      "测试集Acc：0.1001\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_3/coteaching/cifar-resnet18_worker_restore.pth\n",
      "test_acc: 10.03\n",
      "label: 0, acc: 0.00\n",
      "label: 1, acc: 90.70\n",
      "label: 2, acc: 0.00\n",
      "label: 3, acc: 0.00\n",
      "label: 4, acc: 0.00\n",
      "label: 5, acc: 0.00\n",
      "label: 6, acc: 0.00\n",
      "label: 7, acc: 9.60\n",
      "label: 8, acc: 0.00\n",
      "label: 9, acc: 0.00\n",
      "测试集Acc：0.1003\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_0/coteaching_plus/cifar-resnet18_worker_restore.pth\n",
      "test_acc: 27.19\n",
      "label: 0, acc: 0.00\n",
      "label: 1, acc: 35.10\n",
      "label: 2, acc: 2.40\n",
      "label: 3, acc: 0.30\n",
      "label: 4, acc: 23.30\n",
      "label: 5, acc: 6.80\n",
      "label: 6, acc: 60.90\n",
      "label: 7, acc: 38.00\n",
      "label: 8, acc: 75.90\n",
      "label: 9, acc: 29.20\n",
      "测试集Acc：0.2719\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_1/coteaching_plus/cifar-resnet18_worker_restore.pth\n",
      "test_acc: 11.42\n",
      "label: 0, acc: 0.00\n",
      "label: 1, acc: 35.60\n",
      "label: 2, acc: 0.00\n",
      "label: 3, acc: 0.00\n",
      "label: 4, acc: 0.00\n",
      "label: 5, acc: 0.00\n",
      "label: 6, acc: 0.00\n",
      "label: 7, acc: 78.60\n",
      "label: 8, acc: 0.00\n",
      "label: 9, acc: 0.00\n",
      "测试集Acc：0.1142\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_2/coteaching_plus/cifar-resnet18_worker_restore.pth\n",
      "test_acc: 10.01\n",
      "label: 0, acc: 0.00\n",
      "label: 1, acc: 64.40\n",
      "label: 2, acc: 0.00\n",
      "label: 3, acc: 0.00\n",
      "label: 4, acc: 0.00\n",
      "label: 5, acc: 0.00\n",
      "label: 6, acc: 0.00\n",
      "label: 7, acc: 35.70\n",
      "label: 8, acc: 0.00\n",
      "label: 9, acc: 0.00\n",
      "测试集Acc：0.1001\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_3/coteaching_plus/cifar-resnet18_worker_restore.pth\n",
      "test_acc: 10.03\n",
      "label: 0, acc: 0.00\n",
      "label: 1, acc: 90.70\n",
      "label: 2, acc: 0.00\n",
      "label: 3, acc: 0.00\n",
      "label: 4, acc: 0.00\n",
      "label: 5, acc: 0.00\n",
      "label: 6, acc: 0.00\n",
      "label: 7, acc: 9.60\n",
      "label: 8, acc: 0.00\n",
      "label: 9, acc: 0.00\n",
      "测试集Acc：0.1003\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_0/jocor/cifar-resnet18_worker_restore.pth\n",
      "test_acc: 27.19\n",
      "label: 0, acc: 0.00\n",
      "label: 1, acc: 35.10\n",
      "label: 2, acc: 2.40\n",
      "label: 3, acc: 0.30\n",
      "label: 4, acc: 23.30\n",
      "label: 5, acc: 6.80\n",
      "label: 6, acc: 60.90\n",
      "label: 7, acc: 38.00\n",
      "label: 8, acc: 75.90\n",
      "label: 9, acc: 29.20\n",
      "测试集Acc：0.2719\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_1/jocor/cifar-resnet18_worker_restore.pth\n",
      "test_acc: 11.42\n",
      "label: 0, acc: 0.00\n",
      "label: 1, acc: 35.60\n",
      "label: 2, acc: 0.00\n",
      "label: 3, acc: 0.00\n",
      "label: 4, acc: 0.00\n",
      "label: 5, acc: 0.00\n",
      "label: 6, acc: 0.00\n",
      "label: 7, acc: 78.60\n",
      "label: 8, acc: 0.00\n",
      "label: 9, acc: 0.00\n",
      "测试集Acc：0.1142\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_2/jocor/cifar-resnet18_worker_restore.pth\n",
      "test_acc: 10.01\n",
      "label: 0, acc: 0.00\n",
      "label: 1, acc: 64.40\n",
      "label: 2, acc: 0.00\n",
      "label: 3, acc: 0.00\n",
      "label: 4, acc: 0.00\n",
      "label: 5, acc: 0.00\n",
      "label: 6, acc: 0.00\n",
      "label: 7, acc: 35.70\n",
      "label: 8, acc: 0.00\n",
      "label: 9, acc: 0.00\n",
      "测试集Acc：0.1001\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_3/jocor/cifar-resnet18_worker_restore.pth\n",
      "test_acc: 10.03\n",
      "label: 0, acc: 0.00\n",
      "label: 1, acc: 90.70\n",
      "label: 2, acc: 0.00\n",
      "label: 3, acc: 0.00\n",
      "label: 4, acc: 0.00\n",
      "label: 5, acc: 0.00\n",
      "label: 6, acc: 0.00\n",
      "label: 7, acc: 9.60\n",
      "label: 8, acc: 0.00\n",
      "label: 9, acc: 0.00\n",
      "测试集Acc：0.1003\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_0/contra/cifar-resnet18_worker_restore.pth\n",
      "test_acc: 71.17\n",
      "label: 0, acc: 75.40\n",
      "label: 1, acc: 89.20\n",
      "label: 2, acc: 56.70\n",
      "label: 3, acc: 47.70\n",
      "label: 4, acc: 62.20\n",
      "label: 5, acc: 63.20\n",
      "label: 6, acc: 82.00\n",
      "label: 7, acc: 71.90\n",
      "label: 8, acc: 85.80\n",
      "label: 9, acc: 77.60\n",
      "测试集Acc：0.7117\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_1/contra/cifar-resnet18_worker_tta.pth\n",
      "test_acc: 12.46\n",
      "label: 0, acc: 92.40\n",
      "label: 1, acc: 31.70\n",
      "label: 2, acc: 0.00\n",
      "label: 3, acc: 0.00\n",
      "label: 4, acc: 0.00\n",
      "label: 5, acc: 0.00\n",
      "label: 6, acc: 0.00\n",
      "label: 7, acc: 0.50\n",
      "label: 8, acc: 0.00\n",
      "label: 9, acc: 0.00\n",
      "测试集Acc：0.1246\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_2/contra/cifar-resnet18_worker_tta.pth\n",
      "test_acc: 10.07\n",
      "label: 0, acc: 99.80\n",
      "label: 1, acc: 0.00\n",
      "label: 2, acc: 0.00\n",
      "label: 3, acc: 0.70\n",
      "label: 4, acc: 0.00\n",
      "label: 5, acc: 0.00\n",
      "label: 6, acc: 0.00\n",
      "label: 7, acc: 0.10\n",
      "label: 8, acc: 0.10\n",
      "label: 9, acc: 0.00\n",
      "测试集Acc：0.1007\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_3/contra/cifar-resnet18_worker_tta.pth\n",
      "test_acc: 10.00\n",
      "label: 0, acc: 100.00\n",
      "label: 1, acc: 0.00\n",
      "label: 2, acc: 0.00\n",
      "label: 3, acc: 0.00\n",
      "label: 4, acc: 0.00\n",
      "label: 5, acc: 0.00\n",
      "label: 6, acc: 0.00\n",
      "label: 7, acc: 0.00\n",
      "label: 8, acc: 0.00\n",
      "label: 9, acc: 0.00\n",
      "测试集Acc：0.1\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'cifar-10'\n",
    "model_name = 'cifar-resnet18'\n",
    "noise_type='symmetric'\n",
    "\n",
    "eva_test_acc(dataset_name, model_name, noise_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pet-37 分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'pet-37'\n",
    "model_name = 'wideresnet50'\n",
    "noise_type='symmetric'\n",
    "\n",
    "eva_test_acc(dataset_name, model_name, noise_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cifar-100 检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'cifar-100'\n",
    "model_name = 'cifar-wideresnet40'\n",
    "noise_type= 'asymmetric'\n",
    "\n",
    "eva_test_acc(dataset_name, model_name, noise_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pet-37 检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'pet-37'\n",
    "model_name = 'wideresnet50'\n",
    "noise_type= 'asymmetric'\n",
    "\n",
    "eva_test_acc(dataset_name, model_name, noise_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apbench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
