{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 环境导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入库\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 添加项目根目录到 sys.path\n",
    "notebook_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, \"..\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "cotta_root = os.path.join(project_root, 'baseline_code/cotta-main/cifar')\n",
    "sys.path.append(cotta_root)\n",
    "\n",
    "plf_root = os.path.join(project_root, 'baseline_code/PLF-main/cifar')\n",
    "sys.path.append(plf_root)\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "from core_model.custom_model import ClassifierWrapper, load_custom_model\n",
    "from core_model.dataset import get_dataset_loader\n",
    "\n",
    "from core_model.train_test import model_test\n",
    "\n",
    "from core_model.lip_teacher import SimpleLipNet\n",
    "\n",
    "from configs import settings\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import cotta\n",
    "import plf\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from cfgs.conf_cotta import cfg as cfg_cotta\n",
    "from cfgs.conf_plf import cfg as cfg_plf\n",
    "\n",
    "\n",
    "from args_paser import parse_args\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# sys.argv = ['', '--dataset','cifar-10', '--model', 'cifar-resnet18']\n",
    "# custom_args = parse_args()\n",
    "# custom_args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7' # 设置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 通用函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_cotta(model, args):\n",
    "    \"\"\"Set up tent adaptation.\n",
    "\n",
    "    Configure the model for training + feature modulation by batch statistics,\n",
    "    collect the parameters for feature modulation by gradient optimization,\n",
    "    set up the optimizer, and then tent the model.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def setup_optimizer(params):\n",
    "        \"\"\"Set up optimizer for tent adaptation.\n",
    "\n",
    "        Tent needs an optimizer for test-time entropy minimization.\n",
    "        In principle, tent could make use of any gradient optimizer.\n",
    "        In practice, we advise choosing Adam or SGD+momentum.\n",
    "        For optimization settings, we advise to use the settings from the end of\n",
    "        trainig, if known, or start with a low learning rate (like 0.001) if not.\n",
    "\n",
    "        For best results, try tuning the learning rate and batch size.\n",
    "        \"\"\"\n",
    "        if cfg_cotta.OPTIM.METHOD == \"Adam\":\n",
    "            return optim.Adam(\n",
    "                params,\n",
    "                lr=cfg_cotta.OPTIM.LR,\n",
    "                betas=(cfg_cotta.OPTIM.BETA, 0.999),\n",
    "                weight_decay=cfg_cotta.OPTIM.WD,\n",
    "            )\n",
    "        elif cfg_cotta.OPTIM.METHOD == \"SGD\":\n",
    "            return optim.SGD(\n",
    "                params,\n",
    "                lr=cfg_cotta.OPTIM.LR,\n",
    "                momentum=cfg_cotta.OPTIM.MOMENTUM,\n",
    "                dampening=cfg_cotta.OPTIM.DAMPENING,\n",
    "                weight_decay=cfg_cotta.OPTIM.WD,\n",
    "                nesterov=cfg_cotta.OPTIM.NESTEROV,\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    model = cotta.configure_model(model)\n",
    "    params, param_names = cotta.collect_params(model)\n",
    "    optimizer = setup_optimizer(params)\n",
    "    cotta_model = cotta.CoTTA(\n",
    "        model,\n",
    "        optimizer,\n",
    "        args,\n",
    "        steps=cfg_cotta.OPTIM.STEPS,\n",
    "        episodic=cfg_cotta.MODEL.EPISODIC,\n",
    "        mt_alpha=cfg_cotta.OPTIM.MT,\n",
    "        rst_m=cfg_cotta.OPTIM.RST,\n",
    "        ap=cfg_cotta.OPTIM.AP,\n",
    "    )\n",
    "    return cotta_model\n",
    "\n",
    "\n",
    "\n",
    "def setup_plf(model, custom_args, num_classes):\n",
    "    \"\"\"Set up tent adaptation.\n",
    "\n",
    "    Configure the model for training + feature modulation by batch statistics,\n",
    "    collect the parameters for feature modulation by gradient optimization,\n",
    "    set up the optimizer, and then tent the model.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "    def setup_optimizer(params):\n",
    "        \"\"\"Set up optimizer for tent adaptation.\n",
    "\n",
    "        Tent needs an optimizer for test-time entropy minimization.\n",
    "        In principle, tent could make use of any gradient optimizer.\n",
    "        In practice, we advise choosing Adam or SGD+momentum.\n",
    "        For optimization settings, we advise to use the settings from the end of\n",
    "        trainig, if known, or start with a low learning rate (like 0.001) if not.\n",
    "\n",
    "        For best results, try tuning the learning rate and batch size.\n",
    "        \"\"\"\n",
    "        if cfg_plf.OPTIM.METHOD == \"Adam\":\n",
    "            return optim.Adam(\n",
    "                params,\n",
    "                lr=cfg_plf.OPTIM.LR,\n",
    "                betas=(cfg_plf.OPTIM.BETA, 0.999),\n",
    "                weight_decay=cfg_plf.OPTIM.WD,\n",
    "            )\n",
    "        elif cfg_plf.OPTIM.METHOD == \"SGD\":\n",
    "            return optim.SGD(\n",
    "                params,\n",
    "                lr=cfg_plf.OPTIM.LR,\n",
    "                momentum=cfg_plf.OPTIM.MOMENTUM,\n",
    "                dampening=cfg_plf.OPTIM.DAMPENING,\n",
    "                weight_decay=cfg_plf.OPTIM.WD,\n",
    "                nesterov=cfg_plf.OPTIM.NESTEROV,\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "\n",
    "    model = plf.configure_model(model)\n",
    "    params, param_names = plf.collect_params(model)\n",
    "    optimizer = setup_optimizer(params)\n",
    "    plf_model = plf.PLF(\n",
    "        model,\n",
    "        optimizer,\n",
    "        custom_args,\n",
    "        steps=cfg_plf.OPTIM.STEPS,\n",
    "        episodic=cfg_plf.MODEL.EPISODIC,\n",
    "        mt_alpha=cfg_plf.OPTIM.MT,\n",
    "        rst_m=cfg_plf.OPTIM.RST,\n",
    "        ap=cfg_plf.OPTIM.AP,\n",
    "        num_classes=num_classes,\n",
    "    )\n",
    "    return plf_model\n",
    "\n",
    "\n",
    "def clean_accuracy(model: nn.Module,\n",
    "                   x: torch.Tensor,\n",
    "                   y: torch.Tensor,\n",
    "                   batch_size: int = 100,\n",
    "                   device: torch.device = None):\n",
    "    if device is None:\n",
    "        device = x.device\n",
    "    acc = 0.\n",
    "    n_batches = math.ceil(x.shape[0] / batch_size)\n",
    "    with torch.no_grad():\n",
    "        for counter in trange(n_batches):\n",
    "            x_curr = x[counter * batch_size:(counter + 1) *\n",
    "                       batch_size].to(device)\n",
    "            y_curr = y[counter * batch_size:(counter + 1) *\n",
    "                       batch_size].to(device)\n",
    "\n",
    "            # print(f'Batch with sample num: {len(x_curr)}')\n",
    "            output = model(x_curr)\n",
    "            corrected_num = (output.max(1)[1] == y_curr).float().sum()\n",
    "            acc += corrected_num\n",
    "            \n",
    "            # [2024-10-10 sunzekun] 屏蔽了结果输出，保持界面整洁\n",
    "            # print('batch %d, corrected_num: %d' % (counter, corrected_num.item()))\n",
    "        # save step model_tta\n",
    "    return acc.item() / x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_acc(test_loader, model, device):\n",
    "\n",
    "    # Run-Experiment代码里的评估代码。\n",
    "    # 只能测试总体的test_acc\n",
    "    # 放在这里只是为了检查一下错误是不是发生在eva中。实际上可能不用。\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.eval().to(device)   \n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(\n",
    "            total=len(test_loader), desc=f\"Testing\"\n",
    "        ) as pbar:\n",
    "            for test_inputs, test_targets in test_loader:\n",
    "                test_inputs, test_targets = test_inputs.to(device), test_targets.to(\n",
    "                    device\n",
    "                )\n",
    "                test_outputs = model(test_inputs)\n",
    "                loss = criterion(test_outputs, test_targets)\n",
    "                _, predicted_test = torch.max(test_outputs, 1)\n",
    "                total_test += test_targets.size(0)\n",
    "                correct_test += (predicted_test == test_targets).sum().item()\n",
    "\n",
    "                # 更新进度条\n",
    "                pbar.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "                pbar.update(1)\n",
    "\n",
    "    test_accuracy = 100 * correct_test / total_test\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    return test_accuracy\n",
    "        \n",
    "\n",
    "\n",
    "class BaseTensorDataset(Dataset):\n",
    "\n",
    "    # Run-Experiment代码里的自定义数据集。\n",
    "    # 放在这里只是为了检查一下错误是不是发生在数据集中。实际上可能不用。\n",
    "\n",
    "    def __init__(self, data, labels, transforms=None, device=None):\n",
    "        self.data = torch.as_tensor(data, device=device)\n",
    "        self.labels = torch.as_tensor(labels, device=device)\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        if self.transforms is not None:\n",
    "            self.transforms(data)\n",
    "\n",
    "        return data, self.labels[index]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_suffix(method, step):\n",
    "    # 不同的method有不同的后缀\n",
    "    # 例如，contra有restore和tta的，我们在综合测评中应该只考虑tta\n",
    "    # 并且根据step还有所不同。step0的情况下，所有的suffix都为worker_restore.\n",
    "    # 因此，需要一个函数专门处理各种情况\n",
    "    # # Step-0\n",
    "\n",
    "    if step == 0:\n",
    "        # [24-10-11 sunzekun] 只是为了检测一下contra-teacher的性能临时加的代码\n",
    "        # if method == 'contra':\n",
    "        #     return 'teacher_restore'\n",
    "        # else:\n",
    "        #     return \"worker_restore\"\n",
    "        return \"worker_restore\"\n",
    "    \n",
    "    # Step-1,2,3\n",
    "    if method in ['cotta', 'plf', 'contra']:\n",
    "        return \"worker_tta\"\n",
    "    else:\n",
    "        return \"worker_restore\"\n",
    "    \n",
    "\n",
    "    # ================================================================= #\n",
    "    # [24-10-14 sunzekun] 单独为ablation-study写的get_suffix\n",
    "    # 因为区别只是在后缀上精细控制，所以不改eva_系列的函数\n",
    "    # 仅在该函数中进行手动切换，使用的时候需要特别注意\n",
    "\n",
    "    # if step == 0:\n",
    "    #     return \"worker_restore\"\n",
    "    \n",
    "    # # Step-1,2,3\n",
    "    # if method == 'contra_tta_only':\n",
    "    #     return \"worker_restore\"\n",
    "    #     # return \"worker_tta\"\n",
    "    # else: # method = 'contra' \n",
    "    #     return \"worker_tta\" \n",
    "    #     # return \"worker_restore\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eva_test_acc(dataset_name, model_name, noise_type='symmetric', noise_ratio=0.2, methods=None, steps=None, results_dir=None):\n",
    "    \"\"\"\n",
    "    核心代码，用来评估指定dataset任务下的所有方法的step0~4的测试准确率\n",
    "    \"\"\"\n",
    "    \n",
    "    case = settings.get_case(noise_ratio=noise_ratio, noise_type=noise_type, balanced=True)\n",
    "    mean, std = None, None\n",
    "    num_classes = settings.num_classes_dict[dataset_name]\n",
    "    print(f'目前测试的数据集：{dataset_name}, case模式：{case}')\n",
    "\n",
    "    # 读入测试数据集    \n",
    "    # core.py中使用的数据集读入代码\n",
    "    \n",
    "    # batch_size = 128\n",
    "\n",
    "    batch_size = 64\n",
    "    \n",
    "    # PET-37的cotta方法,batch_size要设置的特别小\n",
    "    # batch_size = 1\n",
    "    test_data, test_labels, test_dataloader = get_dataset_loader(\n",
    "        dataset_name, \"test\", case, None, mean, std, batch_size=batch_size, shuffle=False, num_workers=8\n",
    "    )\n",
    "\n",
    "    # run_experiment.py中的代码, 用于对比验证效果。\n",
    "    # print(f'Targeted dataset: {settings.get_dataset_path(dataset_name, case, \"test_data\")}')\n",
    "\n",
    "    # D_test_data = np.load(\n",
    "    #     settings.get_dataset_path(dataset_name, case, \"test_data\")\n",
    "    # )\n",
    "    # D_test_labels = np.load(\n",
    "    #     settings.get_dataset_path(dataset_name, case, \"test_label\")\n",
    "    # )\n",
    "    # test_dataset = BaseTensorDataset(D_test_data, D_test_labels, device=device)\n",
    "    # test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "    # [Debug用]-检查一下数据\n",
    "    # x, y = next(iter(test_dataloader))\n",
    "    # print(x[0])\n",
    "\n",
    "    # \n",
    "    # steps = [f'step_{i}' for i in range(4)]\n",
    "    assert methods is not None, \"请指定要评估的方法\"\n",
    "    assert steps is not None, \"请指定要评估的step(实验组)\"\n",
    "    assert methods is not None, \"请指定评估结果保存的目录\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    df = pd.DataFrame(index=methods, columns=steps)\n",
    "\n",
    "\n",
    "    # for method in tqdm(methods):\n",
    "    \n",
    "    for method in methods:\n",
    "        for step in steps:\n",
    "            # 先确定一下模型的suffix\n",
    "            model_suffix = get_suffix(method, step)\n",
    "            # 读入模型架构\n",
    "            # 放到循环里边，每次都要重新新建一个模型。\n",
    "            # 牺牲了一些速度，但是好处是防止cotta和plf把模型架构更改了，导致repair类的模型出问题\n",
    "            model = load_custom_model(model_name, num_classes, load_pretrained=False)\n",
    "            model = ClassifierWrapper(model, num_classes)\n",
    "\n",
    "            # 按照模型名和step数读入模型参数\n",
    "            model_repair_save_path = settings.get_ckpt_path(dataset_name, case, model_name, model_suffix=model_suffix, step=step, unique_name=method)         \n",
    "            \"\"\"\n",
    "            专为测试lipNet性能的代码\n",
    "            \"\"\"\n",
    "            # model = load_custom_model(model_name, num_classes, load_pretrained=True)\n",
    "            # features = model.fc.in_features\n",
    "            # model = nn.Sequential(*list(model.children())[:-1], nn.Flatten())\n",
    "            # model = SimpleLipNet(model, features, num_classes)            \n",
    "            # model_repair_save_path = '/nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/pretrain/step_0/cifar-resnet18_teacher_restore.pth'\n",
    "            # model_repair_save_path = '/nvme/szh/code/tta-mr/ckpt/cifar-10/pretrain/step_0/cifar-resnet18_teacher_restore.pth'\n",
    "            # model_repair_save_path = '/nvme/szh/code/tta-mr/ckpt/cifar-10/pretrain/step_0/cifar-resnet18_worker_restore.pth'\n",
    "            # model_repair_save_path = '/nvme/sunzekun/Projects/tta-mr/ckpt/cifar-100/pretrain/step_0/cifar-wideresnet40_teacher_restore.pth'\n",
    "            # model_repair_save_path = '/nvme/sunzekun/Projects/tta-mr/ckpt/cifar-100/pretrain/step_0/cifar-wideresnet40_worker_restore.pth'\n",
    "            # model_repair_save_path = '/nvme/sunzekun/Projects/tta-mr/ckpt/pet-37/pretrain/step_0/wideresnet50_teacher_restore.pth'\n",
    "            # model_repair_save_path = '/nvme/sunzekun/Projects/tta-mr/ckpt/pet-37/pretrain/step_0/wideresnet50_worker_restore.pth'\n",
    "\n",
    "            print(f'Evaluating {model_repair_save_path}')\n",
    "\n",
    "            # checkpoint = torch.load(model_repair_save_path)\n",
    "            try:\n",
    "                checkpoint = torch.load(model_repair_save_path)\n",
    "            except:\n",
    "                print(f\"Cannot find the weight file at {model_repair_save_path}. Just SKIP.\")\n",
    "                continue\n",
    "            model.load_state_dict(checkpoint, strict=False)\n",
    "\n",
    "\n",
    "            # [24-10-10 sunzekun] 有两个特殊的tta模型：cotta和plf，不仅改了参数，而且使用了特定的模型架构来进行推断。\n",
    "            # (即，不仅有test-time Adaptation，还有Augmentation)\n",
    "            # 所以对这两种需要另外写代码实现。\n",
    "            # 注意，contra没有这个特殊过程。\n",
    "            if method == 'cotta' or method == 'plf':\n",
    "            # if (method == 'cotta' or method == 'plf') and dataset_name != 'pet-37':\n",
    "            # if (method == 'cotta' or method == 'plf') and dataset_name == 'cifar-10':\n",
    "                # 由于测试代码的jupyternotebook 构建命令行参数很麻烦\n",
    "                # 这里暂时去掉了dataset，model这两个必选参数的required = True\n",
    "                # 而是直接在使用的时候复制。\n",
    "                sys.argv = ['', '--dataset', dataset_name, '--model', model_name, '--uni_name', method, '--noise_type', noise_type, '--balanced']\n",
    "                custom_args = parse_args()\n",
    "                model.eval().to(device)\n",
    "                if method == 'cotta':\n",
    "                    model_aug = setup_cotta(model, custom_args)\n",
    "                else:\n",
    "                    model_aug = setup_plf(model, custom_args, num_classes)\n",
    "                \n",
    "                try:\n",
    "                    model_aug.reset()\n",
    "                except:\n",
    "                    print(f'Failed to reset')\n",
    "                \n",
    "                x_test = torch.from_numpy(test_data)\n",
    "                y_test = torch.from_numpy(test_labels)\n",
    "                x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "\n",
    "                test_acc = clean_accuracy(model_aug, x_test, y_test, batch_size=batch_size, device=device)                \n",
    "                # print(test_acc)\n",
    "            else:\n",
    "                test_acc = model_test(test_dataloader, model, device=device)\n",
    "                # print(test_acc)\n",
    "                test_acc = test_acc['global']\n",
    "            \n",
    "            print(f\"测试集Acc：{test_acc}\")\n",
    "            df.loc[method, step] = test_acc\n",
    "            # test_acc = get_test_acc(test_dataloader, model, device=device)\n",
    "            # df.loc[method, step] = test_acc\n",
    "\n",
    "\n",
    "\n",
    "    # 保留参考代码，万一想用excel保存结果的时候用下面的\n",
    "    # results_file_name = f'{dataset_name}.xlsx'\n",
    "    # results_file_dir = os.path.join(results_dir, results_file_name)\n",
    "    # with pd.ExcelWriter(results_file_dir, engine='openpyxl') as writer:\n",
    "    #     df.to_excel(writer)\n",
    "        # for tab, df in dfs.items():\n",
    "\n",
    "    # cls: 分类任务\n",
    "    # rtv: 检索任务\n",
    "    mission_type = 'cls' if noise_type == 'symmetric' else 'rtv'\n",
    "    results_file_name = f'{dataset_name}_{mission_type}.csv'\n",
    "    results_file_dir = os.path.join(results_dir, results_file_name)\n",
    "    df.to_csv(results_file_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eva_map(dataset_name, model_name, noise_type='asymmetric', noise_ratio=0.2, top_k=10, methods=None, steps=None, results_dir=None):\n",
    "    \"\"\"\n",
    "    核心代码，用来评估指定dataset任务下的所有方法的step0~4的map，检索任务专用\n",
    "    \"\"\"\n",
    "    \n",
    "    case = settings.get_case(noise_ratio=noise_ratio, noise_type=noise_type, balanced=True)\n",
    "    mean, std = None, None\n",
    "    num_classes = settings.num_classes_dict[dataset_name]\n",
    "    print(f'目前测试的数据集：{dataset_name}, case模式：{case}')\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------------------------------ #\n",
    "    # 读入测试数据集    \n",
    "    # core.py中使用的数据集读入代码\n",
    "    # train_data, train_labels, train_dataloader = get_dataset_loader(\n",
    "    #     dataset_name, \"train\", case, None, mean, std, batch_size=128, shuffle=False, num_workers=8\n",
    "    # )\n",
    "\n",
    "    # test_data, test_labels, test_dataloader = get_dataset_loader(\n",
    "    #     dataset_name, \"test\", case, None, mean, std, batch_size=128, shuffle=False, num_workers=8\n",
    "    # )\n",
    "\n",
    "    # [24-10-14 数据集存放位置有变化，现在train和test的data已经独立于case了，所以去掉]\n",
    "    train_data, train_labels, train_dataloader = get_dataset_loader(\n",
    "        dataset_name, \"train\", None, None, mean, std, batch_size=128, shuffle=False, num_workers=8\n",
    "    )\n",
    "\n",
    "    test_data, test_labels, test_dataloader = get_dataset_loader(\n",
    "        dataset_name, \"test\", None, None, mean, std, batch_size=128, shuffle=False, num_workers=8\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------------------------------ #\n",
    "    \n",
    "    # run_experiment.py中的代码, 用于对比验证效果。\n",
    "    # print(f'Targeted dataset: {settings.get_dataset_path(dataset_name, case, \"test_data\")}')\n",
    "\n",
    "    # D_test_data = np.load(\n",
    "    #     settings.get_dataset_path(dataset_name, case, \"test_data\")\n",
    "    # )\n",
    "    # D_test_labels = np.load(\n",
    "    #     settings.get_dataset_path(dataset_name, case, \"test_label\")\n",
    "    # )\n",
    "    # test_dataset = BaseTensorDataset(D_test_data, D_test_labels, device=device)\n",
    "    # test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "    # [Debug用]-检查一下数据\n",
    "    # x, y = next(iter(test_dataloader))\n",
    "    # print(x[0])\n",
    "\n",
    "    assert methods is not None, \"请指定要评估的方法\"\n",
    "    assert steps is not None, \"请指定要评估的step(实验组)\"\n",
    "    assert methods is not None, \"请指定评估结果保存的目录\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    df = pd.DataFrame(index=methods, columns=steps)\n",
    "\n",
    "\n",
    "    # for method in tqdm(methods):\n",
    "    \n",
    "    for method in methods:\n",
    "        for step in steps:\n",
    "            # 先确定一下模型的suffix\n",
    "            model_suffix = get_suffix(method, step)\n",
    "            # 读入模型架构\n",
    "            # 放到循环里边，每次都要重新新建一个模型。\n",
    "            # 牺牲了一些速度，但是好处是防止cotta和plf把模型架构更改了，导致repair类的模型出问题\n",
    "            model = load_custom_model(model_name, num_classes, load_pretrained=False)\n",
    "            model = ClassifierWrapper(model, num_classes)\n",
    "\n",
    "            # 按照模型名和step数读入模型参数\n",
    "            model_repair_save_path = settings.get_ckpt_path(dataset_name, case, model_name, model_suffix=model_suffix, step=step, unique_name=method)\n",
    "            print(f'Evaluating {model_repair_save_path}')\n",
    "\n",
    "            # checkpoint = torch.load(model_repair_save_path)\n",
    "            try:\n",
    "                checkpoint = torch.load(model_repair_save_path)\n",
    "            except:\n",
    "                print(f\"Cannot find the weight file at {model_repair_save_path}. Just SKIP.\")\n",
    "                continue\n",
    "            model.load_state_dict(checkpoint, strict=False)\n",
    "\n",
    "\n",
    "            # [24-10-10 sunzekun] 有两个特殊的tta模型：cotta和plf，不仅改了参数，而且使用了特定的模型架构来进行推断。\n",
    "            # (即，不仅有test-time Adaptation，还有Augmentation)\n",
    "            # 所以对这两种需要另外写代码实现。\n",
    "            # 注意，contra没有这个特殊过程。\n",
    "\n",
    "            # [24-10-11 sunzekun] 对于两个带tta的模型，这里不再做单独的增强了，因为增强和检索比较难适配。\n",
    "            # 直接 \n",
    "\n",
    "            mAP = get_map(model, train_dataloader, test_dataloader, top_k)            \n",
    "            print(f\"测试mAP：{mAP}\")\n",
    "\n",
    "            \n",
    "            df.loc[method, step] = mAP\n",
    "            # test_acc = get_test_acc(test_dataloader, model, device=device)\n",
    "            # df.loc[method, step] = test_acc\n",
    "\n",
    "    # 保留参考代码，万一想用excel保存结果的时候用下面的\n",
    "    # results_file_name = f'{dataset_name}.xlsx'\n",
    "    # results_file_dir = os.path.join(results_dir, results_file_name)\n",
    "    # with pd.ExcelWriter(results_file_dir, engine='openpyxl') as writer:\n",
    "    #     df.to_excel(writer)\n",
    "        # for tab, df in dfs.items():\n",
    "\n",
    "    # cls: 分类任务\n",
    "    # rtv: 检索任务\n",
    "    mission_type = 'cls' if noise_type == 'symmetric' else 'rtv'\n",
    "    results_file_name = f'{dataset_name}_{mission_type}.csv'\n",
    "    results_file_dir = os.path.join(results_dir, results_file_name)\n",
    "    df.to_csv(results_file_dir)\n",
    "\n",
    "\n",
    "\n",
    "# 特征提取函数\n",
    "def extract_features(feature_extractor, data_loader):\n",
    "    features = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(data_loader):\n",
    "            images = images.to(device)\n",
    "            outputs = feature_extractor(images)\n",
    "            outputs = outputs.view(outputs.size(0), -1)\n",
    "            features.append(outputs.cpu().numpy())\n",
    "            labels.append(targets.numpy())\n",
    "    features = np.concatenate(features)\n",
    "    labels = np.concatenate(labels)\n",
    "    return features, labels\n",
    "\n",
    "def retrieve(gallery_feats, query_feats, top_k=10):\n",
    "    sims = cosine_similarity(query_feats, gallery_feats)\n",
    "    indices = np.argsort(-sims, axis=1)[:, :top_k]\n",
    "    sim_scores = np.take_along_axis(sims, indices, axis=1)\n",
    "    return indices, sim_scores\n",
    "\n",
    "\n",
    "\n",
    "def calculate_map(indices, gallery_labels, query_labels):\n",
    "    num_queries = query_labels.shape[0]\n",
    "    ap_list = []\n",
    "    for i in range(num_queries):\n",
    "        query_label = query_labels[i]\n",
    "        retrieved_labels = gallery_labels[indices[i]]\n",
    "        relevant = (retrieved_labels == query_label).astype(int)\n",
    "        num_relevant = relevant.sum()\n",
    "        if num_relevant == 0:\n",
    "            ap_list.append(0)\n",
    "            continue\n",
    "        cumulative_precision = np.cumsum(relevant) / (np.arange(len(relevant)) + 1)\n",
    "        ap = (cumulative_precision * relevant).sum() / num_relevant\n",
    "        ap_list.append(ap)\n",
    "    mAP = np.mean(ap_list)\n",
    "    return mAP\n",
    "\n",
    "def get_map(model, train_loader, test_loader, top_k):\n",
    "    model.to(device)\n",
    "    model.eval()    \n",
    "    feature_extractor = nn.Sequential(*list(model.children())[:-1])\n",
    "    feature_extractor = feature_extractor.to(device)\n",
    "    feature_extractor.eval()\n",
    "\n",
    "    # 提取特征\n",
    "    g_features, g_labels = extract_features(feature_extractor, train_loader)\n",
    "    q_features, q_labels = extract_features(feature_extractor, test_loader)\n",
    "    indices, _ = retrieve(g_features, q_features, top_k=top_k)\n",
    "    mAP = calculate_map(indices, g_labels, q_labels)\n",
    "\n",
    "    return mAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eva_retrieval(dataset_name, model_name, noise_type='asymmetric', noise_ratio=0.2, methods=None, steps=None, top_k=5, query_indices=[0], use_cache=False):\n",
    "    \"\"\"\n",
    "    核心代码，用来展示任意模型的检索结果，检索任务专用\n",
    "    重要参数：top_k, 用于确定一次输出多少张图片\n",
    "    \"\"\"\n",
    "    \n",
    "    case = settings.get_case(noise_ratio=noise_ratio, noise_type=noise_type, balanced=True)\n",
    "    mean, std = None, None\n",
    "    num_classes = settings.num_classes_dict[dataset_name]\n",
    "    print(f'目前测试的数据集：{dataset_name}, case模式：{case}')\n",
    "\n",
    "    # 读入测试数据集    \n",
    "    # core.py中使用的数据集读入代码\n",
    "    # train_data, train_labels, train_dataloader = get_dataset_loader(\n",
    "    #     dataset_name, \"train\", case, None, mean, std, batch_size=128, shuffle=False\n",
    "    # )\n",
    "\n",
    "    # # TODO-仅用来测试一下数据集的归一化和恢复，以及展示情况\n",
    "    # # print(train_data[0].shape)\n",
    "    # # print(train_data[0])\n",
    "    # # return\n",
    "    # # img = recover_img(train_data[0], dataset_name)\n",
    "    # # print(img.max(), img.min())\n",
    "    # # plt.figure()\n",
    "    # # plt.imshow(np.transpose(img, (1, 2, 0)))\n",
    "    # # plt.show()\n",
    "    # # return\n",
    "\n",
    "    # test_data, test_labels, test_dataloader = get_dataset_loader(\n",
    "    #     dataset_name, \"test\", case, None, mean, std, batch_size=128, shuffle=False\n",
    "    # )\n",
    "\n",
    "\n",
    "\n",
    "    train_data, train_labels, train_dataloader = get_dataset_loader(\n",
    "        dataset_name, \"train\", None, None, mean, std, batch_size=128, shuffle=False\n",
    "    )\n",
    "\n",
    "    test_data, test_labels, test_dataloader = get_dataset_loader(\n",
    "        dataset_name, \"test\", None, None, mean, std, batch_size=128, shuffle=False\n",
    "    )\n",
    "\n",
    "    # run_experiment.py中的代码, 用于对比验证效果。\n",
    "    # print(f'Targeted dataset: {settings.get_dataset_path(dataset_name, case, \"test_data\")}')\n",
    "\n",
    "    # D_test_data = np.load(\n",
    "    #     settings.get_dataset_path(dataset_name, case, \"test_data\")\n",
    "    # )\n",
    "    # D_test_labels = np.load(\n",
    "    #     settings.get_dataset_path(dataset_name, case, \"test_label\")\n",
    "    # )\n",
    "    # test_dataset = BaseTensorDataset(D_test_data, D_test_labels, device=device)\n",
    "    # test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "    # [Debug用]-检查一下数据\n",
    "    # x, y = next(iter(test_dataloader))\n",
    "    # print(x[0])\n",
    "    \n",
    "    assert methods is not None, \"请指定要评估的方法\"\n",
    "    assert steps is not None, \"请指定要评估的step(实验组)\"\n",
    "\n",
    "\n",
    "\n",
    "    # for method in tqdm(methods):\n",
    "    \n",
    "    for method in methods:\n",
    "        for step in steps:\n",
    "            # 先确定一下模型的suffix\n",
    "            model_suffix = get_suffix(method, step)\n",
    "            # 读入模型架构\n",
    "            # 放到循环里边，每次都要重新新建一个模型。\n",
    "            # 牺牲了一些速度，但是好处是防止cotta和plf把模型架构更改了，导致repair类的模型出问题\n",
    "            model = load_custom_model(model_name, num_classes, load_pretrained=False)\n",
    "            model = ClassifierWrapper(model, num_classes)\n",
    "\n",
    "            # 按照模型名和step数读入模型参数\n",
    "            model_repair_save_path = settings.get_ckpt_path(dataset_name, case, model_name, model_suffix=model_suffix, step=step, unique_name=method)\n",
    "            print(f'Evaluating {model_repair_save_path}')\n",
    "\n",
    "            # checkpoint = torch.load(model_repair_save_path)\n",
    "            try:\n",
    "                checkpoint = torch.load(model_repair_save_path)\n",
    "            except:\n",
    "                print(f\"Cannot find the weight file at {model_repair_save_path}. Just SKIP.\")\n",
    "                continue\n",
    "            model.load_state_dict(checkpoint, strict=False)\n",
    "\n",
    "\n",
    "            # [24-10-10 sunzekun] 有两个特殊的tta模型：cotta和plf，不仅改了参数，而且使用了特定的模型架构来进行推断。\n",
    "            # (即，不仅有test-time Adaptation，还有Augmentation)\n",
    "            # 所以对这两种需要另外写代码实现。\n",
    "            # 注意，contra没有这个特殊过程。\n",
    "\n",
    "            # [24-10-11 sunzekun] 对于两个带tta的模型，这里不再做单独的增强了，因为增强和检索比较难适配。\n",
    "            # 直接 \n",
    "            \n",
    "            cache_path = get_cache_path(dataset_name=dataset_name, case=case, step=step, unique_name=method, top_k=top_k)\n",
    "            # print(cache_path)\n",
    "            # return\n",
    "            if use_cache:\n",
    "                try:\n",
    "                    data = np.load(cache_path)\n",
    "                except:\n",
    "                    print(\"未读取到对应的相似分数缓存。首次评估某个模型时，请先使用use_cache=False\")\n",
    "                    continue\n",
    "                \n",
    "                print(f'读取缓存的相似度分数记录：{cache_path}')\n",
    "                indices = data['indices']\n",
    "                sim_scores = data['sim_scores']\n",
    "            else:\n",
    "                indices, sim_scores = get_sim_scores(model, train_dataloader, test_dataloader, top_k)\n",
    "                np.savez(cache_path, indices=indices, sim_scores=sim_scores)\n",
    "            # print(indices, sim_scores)\n",
    "            # print(f'Length of retrived indices: {len(indices)}')\n",
    "            # return\n",
    "            for query_idx in query_indices:\n",
    "                print(f\"展示Query图片-idx-{query_idx}的搜索结果\")\n",
    "                display_image_grid(\n",
    "                    test_data, test_labels,\n",
    "                    train_data, train_labels,\n",
    "                    query_idx, indices[query_idx], sim_scores[query_idx],\n",
    "                    save_path=None,  # 如果需要保存图像，设置保存路径\n",
    "                    dataset_name=dataset_name  # 用于读取图片的归一化参数，执行‘反归一化’\n",
    "                )\n",
    "\n",
    "def get_sim_scores(model, train_loader, test_loader, top_k):\n",
    "    model.to(device)\n",
    "    model.eval()    \n",
    "    feature_extractor = nn.Sequential(*list(model.children())[:-1])\n",
    "    feature_extractor = feature_extractor.to(device)\n",
    "    feature_extractor.eval()\n",
    "\n",
    "    # 提取特征\n",
    "    g_features, g_labels = extract_features(feature_extractor, train_loader)\n",
    "    q_features, q_labels = extract_features(feature_extractor, test_loader)\n",
    "    indices, sim_scores = retrieve(g_features, q_features, top_k=top_k)\n",
    "    \n",
    "\n",
    "    return indices, sim_scores\n",
    "\n",
    "\n",
    "def draw_box(ax, img, color='red'):\n",
    "    \n",
    "    # TODO- Pet37的参数\n",
    "    # border_width = 6  # 边框的宽度\n",
    "    # # adjust = border_width /2 + 1\n",
    "    # adjust = 2\n",
    "\n",
    "    # TODO- Cifar100的参数\n",
    "    border_width = 6  # 边框的宽度\n",
    "    # adjust = border_width /2 + 1\n",
    "    adjust = 0\n",
    "    \n",
    "    \n",
    "    _, height, width = img.shape\n",
    "    # rect = Rectangle((-adjust + 1 + border_width, -adjust + 1 + border_width), width - 2 * border_width + adjust + 1 , height - 2 * border_width + adjust  + 1 ,\n",
    "    #                 fill=False, edgecolor=color, linewidth=border_width)\n",
    "    # rect = Rectangle((-adjust + border_width, -adjust + border_width), width - 2 * border_width + adjust, height - 2 * border_width + adjust,\n",
    "    #             fill=False, edgecolor=color, linewidth=border_width)\n",
    "    \n",
    "    rect = Rectangle((0, 0), width-1, height-1,\n",
    "            fill=False, edgecolor=color, linewidth=border_width)\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "box_color = {\n",
    "    # 'query' : '#FA9248',\n",
    "    \n",
    "    'query' : 'purple',\n",
    "    'correct': '#4DA764',\n",
    "    'wrong': '#E05757'\n",
    "}\n",
    "\n",
    "\n",
    "# box_color = {\n",
    "#     'query' : 'purple',\n",
    "#     'correct': 'green',\n",
    "#     'wrong': 'red'\n",
    "# }\n",
    "\n",
    "def display_image_grid(query_data, query_labels, gallery_data, gallery_labels, query_index, retrieved_indices, sim_scores, save_path=None, dataset_name=None):\n",
    "    assert dataset_name is not None, \"请指明数据集名称，用于恢复对应的归一化后的图片\"\n",
    "\n",
    "    # 读入class_names，用于展示在图片上\n",
    "    if dataset_name == 'cifar-100':\n",
    "        class_names = load_classes_from_file('../configs/classes/cifar_100_classes.txt')\n",
    "    elif dataset_name == 'pet-37':\n",
    "        class_names = load_classes_from_file('../configs/classes/pet_37_classes.txt')\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    title_fs = 15 # 标题的字号大小\n",
    "    \n",
    "    K = len(retrieved_indices)\n",
    "    plt.style.use(\"default\")\n",
    "    fig, axs = plt.subplots(1, K + 1, figsize=(15, 3))\n",
    "    # 显示查询图像\n",
    "    query_img = query_data[query_index]\n",
    "    query_img = recover_img(query_img, dataset_name)\n",
    "\n",
    "    axs[0].imshow(np.transpose(query_img, (1, 2, 0)))\n",
    "\n",
    "    query_label = query_labels[query_index]\n",
    "    query_class_name = class_names[query_label]\n",
    "    axs[0].set_title(query_class_name, fontsize=title_fs, fontfamily=\"serif\", weight=\"bold\", color=box_color['query'])\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    draw_box(axs[0], query_img, color=box_color['query'])\n",
    "\n",
    "    print(f'查询图片label：{query_label}-{query_class_name}')\n",
    "\n",
    "    # 显示检索结果\n",
    "    for i, idx in enumerate(retrieved_indices):\n",
    "        img = gallery_data[idx]\n",
    "        img = recover_img(img, dataset_name)\n",
    "        axs[i + 1].imshow(np.transpose(img, (1, 2, 0)))\n",
    "        \n",
    "        label = gallery_labels[idx]\n",
    "        class_name = class_names[label]\n",
    "        # TODO [24-10-14 sunzekun] 增加相似度分数显示\n",
    "        class_name = f'{class_name} ({sim_scores[i]:.2f})'\n",
    "\n",
    "        # axs[i + 1].set_title(class_name, fontsize=title_fs, fontfamily=\"serif\", weight=\"bold\")\n",
    "        # axs[i + 1].set_title(f\"Sim: {sim_scores[i]:.2f}\")\n",
    "        axs[i + 1].axis('off')\n",
    "\n",
    "        if label == query_label:\n",
    "            draw_box(axs[i + 1], img, color=box_color['correct'])\n",
    "            axs[i + 1].set_title(class_name, fontsize=title_fs, fontfamily=\"serif\", weight=\"bold\", color=box_color['correct'])\n",
    "        else:\n",
    "            draw_box(axs[i + 1], img, color=box_color['wrong'])\n",
    "            axs[i + 1].set_title(class_name, fontsize=title_fs, fontfamily=\"serif\", weight=\"bold\", color=box_color['wrong'])\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "\n",
    "    plt.gca().set_axis_off()        \n",
    "    plt.subplots_adjust(left=0, right=1, top=1, bottom=0, wspace=0.02, hspace=0)  # 适配top-5\n",
    "    # plt.subplots_adjust(left=0, right=1, top=1, bottom=0, wspace=0.035, hspace=0)  # top-10\n",
    "    # plt.subplots_adjust(left=0, right=1, top=1, bottom=0, wspace=0.0, hspace=0)  # 移除所有边缘空白\n",
    "    # plt.tight_layout()\n",
    "    plt.margins(0,0)    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def recover_img(img, dataset_name):\n",
    "    if dataset_name == 'pet-37':\n",
    "        means = np.array([0.485, 0.456, 0.406])[:, np.newaxis, np.newaxis]\n",
    "        stds = np.array([0.229, 0.224, 0.225])[:, np.newaxis, np.newaxis]\n",
    "    elif dataset_name == 'cifar-100':\n",
    "\n",
    "        means = np.array([0.5071, 0.4865, 0.4409])[:, np.newaxis, np.newaxis]\n",
    "        stds = np.array([0.2673, 0.2564, 0.2762])[:, np.newaxis, np.newaxis]\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return np.clip(img * stds + means, 0, 1)\n",
    "\n",
    "\n",
    "def get_cache_path(dataset_name, case, step, unique_name, top_k):\n",
    "    root_dir = os.path.abspath(os.path.join(os.getcwd(), os.path.pardir))\n",
    "    \n",
    "    path = os.path.join(root_dir, \"result_analysis/results_main/rtv_cache\", dataset_name, case, f\"step_{step}\", unique_name)\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    return os.path.join(path, f\"top_{top_k}.npz\")\n",
    "\n",
    "\n",
    "def load_classes_from_file(file_path):\n",
    "    \"\"\"从文件中读取类别列表\"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        classes = [line.strip() for line in f.readlines()]\n",
    "    return classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PET-37的归一化参数（从gen_pet37的代码里找到）\n",
    "# import torchvision\n",
    "# torchvision.models.ResNet18_Weights.DEFAULT.transforms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主实验\n",
    "### cifar-10-分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目前测试的数据集：cifar-10, case模式：nr_0.2_nt_symmetric_balanced\n",
      "Loading /nvme/sunzekun/Projects/tta-mr/data/cifar-10/gen/nr_0.2_nt_symmetric_balanced/test_data.npy\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_0/raw/cifar-resnet18_worker_restore.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_252992/4186978523.py:83: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_repair_save_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 82.23\n",
      "label: 0, acc: 87.30\n",
      "label: 1, acc: 95.40\n",
      "label: 2, acc: 72.80\n",
      "label: 3, acc: 59.30\n",
      "label: 4, acc: 76.50\n",
      "label: 5, acc: 74.50\n",
      "label: 6, acc: 88.10\n",
      "label: 7, acc: 86.30\n",
      "label: 8, acc: 91.80\n",
      "label: 9, acc: 90.30\n",
      "测试集Acc：0.8223\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_1/raw/cifar-resnet18_worker_restore.pth\n",
      "test_acc: 81.10\n",
      "label: 0, acc: 81.90\n",
      "label: 1, acc: 91.60\n",
      "label: 2, acc: 71.20\n",
      "label: 3, acc: 65.90\n",
      "label: 4, acc: 81.00\n",
      "label: 5, acc: 73.10\n",
      "label: 6, acc: 83.50\n",
      "label: 7, acc: 83.80\n",
      "label: 8, acc: 89.40\n",
      "label: 9, acc: 89.60\n",
      "测试集Acc：0.811\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_2/raw/cifar-resnet18_worker_restore.pth\n",
      "test_acc: 80.80\n",
      "label: 0, acc: 81.80\n",
      "label: 1, acc: 90.10\n",
      "label: 2, acc: 72.10\n",
      "label: 3, acc: 63.40\n",
      "label: 4, acc: 78.50\n",
      "label: 5, acc: 69.90\n",
      "label: 6, acc: 86.40\n",
      "label: 7, acc: 86.10\n",
      "label: 8, acc: 90.10\n",
      "label: 9, acc: 89.60\n",
      "测试集Acc：0.808\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_3/raw/cifar-resnet18_worker_restore.pth\n",
      "test_acc: 76.75\n",
      "label: 0, acc: 82.10\n",
      "label: 1, acc: 88.60\n",
      "label: 2, acc: 72.40\n",
      "label: 3, acc: 46.30\n",
      "label: 4, acc: 80.40\n",
      "label: 5, acc: 67.00\n",
      "label: 6, acc: 84.50\n",
      "label: 7, acc: 75.70\n",
      "label: 8, acc: 89.20\n",
      "label: 9, acc: 81.30\n",
      "测试集Acc：0.7675\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_0/replay/cifar-resnet18_worker_restore.pth\n",
      "test_acc: 82.23\n",
      "label: 0, acc: 87.30\n",
      "label: 1, acc: 95.40\n",
      "label: 2, acc: 72.80\n",
      "label: 3, acc: 59.30\n",
      "label: 4, acc: 76.50\n",
      "label: 5, acc: 74.50\n",
      "label: 6, acc: 88.10\n",
      "label: 7, acc: 86.30\n",
      "label: 8, acc: 91.80\n",
      "label: 9, acc: 90.30\n",
      "测试集Acc：0.8223\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_1/replay/cifar-resnet18_worker_restore.pth\n",
      "test_acc: 82.31\n",
      "label: 0, acc: 92.40\n",
      "label: 1, acc: 89.20\n",
      "label: 2, acc: 76.00\n",
      "label: 3, acc: 48.80\n",
      "label: 4, acc: 87.20\n",
      "label: 5, acc: 70.50\n",
      "label: 6, acc: 91.40\n",
      "label: 7, acc: 82.10\n",
      "label: 8, acc: 93.40\n",
      "label: 9, acc: 92.10\n",
      "测试集Acc：0.8231\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_2/replay/cifar-resnet18_worker_restore.pth\n",
      "test_acc: 80.91\n",
      "label: 0, acc: 93.40\n",
      "label: 1, acc: 87.90\n",
      "label: 2, acc: 76.90\n",
      "label: 3, acc: 43.40\n",
      "label: 4, acc: 84.70\n",
      "label: 5, acc: 65.00\n",
      "label: 6, acc: 92.70\n",
      "label: 7, acc: 79.40\n",
      "label: 8, acc: 94.80\n",
      "label: 9, acc: 90.90\n",
      "测试集Acc：0.8091\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_3/replay/cifar-resnet18_worker_restore.pth\n",
      "test_acc: 78.66\n",
      "label: 0, acc: 93.20\n",
      "label: 1, acc: 88.20\n",
      "label: 2, acc: 78.30\n",
      "label: 3, acc: 32.90\n",
      "label: 4, acc: 85.70\n",
      "label: 5, acc: 57.20\n",
      "label: 6, acc: 93.20\n",
      "label: 7, acc: 75.40\n",
      "label: 8, acc: 94.40\n",
      "label: 9, acc: 88.10\n",
      "测试集Acc：0.7866\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_0/Coteaching/cifar-resnet18_worker_restore.pth\n",
      "test_acc: 82.23\n",
      "label: 0, acc: 87.30\n",
      "label: 1, acc: 95.40\n",
      "label: 2, acc: 72.80\n",
      "label: 3, acc: 59.30\n",
      "label: 4, acc: 76.50\n",
      "label: 5, acc: 74.50\n",
      "label: 6, acc: 88.10\n",
      "label: 7, acc: 86.30\n",
      "label: 8, acc: 91.80\n",
      "label: 9, acc: 90.30\n",
      "测试集Acc：0.8223\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_1/Coteaching/cifar-resnet18_worker_restore.pth\n",
      "test_acc: 80.71\n",
      "label: 0, acc: 83.50\n",
      "label: 1, acc: 92.20\n",
      "label: 2, acc: 72.60\n",
      "label: 3, acc: 59.30\n",
      "label: 4, acc: 78.40\n",
      "label: 5, acc: 72.20\n",
      "label: 6, acc: 85.00\n",
      "label: 7, acc: 83.50\n",
      "label: 8, acc: 90.50\n",
      "label: 9, acc: 89.90\n",
      "测试集Acc：0.8071\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_2/Coteaching/cifar-resnet18_worker_restore.pth\n",
      "test_acc: 79.85\n",
      "label: 0, acc: 85.70\n",
      "label: 1, acc: 90.00\n",
      "label: 2, acc: 71.10\n",
      "label: 3, acc: 57.80\n",
      "label: 4, acc: 82.30\n",
      "label: 5, acc: 70.30\n",
      "label: 6, acc: 83.50\n",
      "label: 7, acc: 84.80\n",
      "label: 8, acc: 85.60\n",
      "label: 9, acc: 87.40\n",
      "测试集Acc：0.7985\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_3/Coteaching/cifar-resnet18_worker_restore.pth\n",
      "test_acc: 77.10\n",
      "label: 0, acc: 83.80\n",
      "label: 1, acc: 86.70\n",
      "label: 2, acc: 74.30\n",
      "label: 3, acc: 50.30\n",
      "label: 4, acc: 77.10\n",
      "label: 5, acc: 65.00\n",
      "label: 6, acc: 85.10\n",
      "label: 7, acc: 78.30\n",
      "label: 8, acc: 87.50\n",
      "label: 9, acc: 82.90\n",
      "测试集Acc：0.771\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_0/Coteachingplus/cifar-resnet18_worker_restore.pth\n",
      "test_acc: 82.23\n",
      "label: 0, acc: 87.30\n",
      "label: 1, acc: 95.40\n",
      "label: 2, acc: 72.80\n",
      "label: 3, acc: 59.30\n",
      "label: 4, acc: 76.50\n",
      "label: 5, acc: 74.50\n",
      "label: 6, acc: 88.10\n",
      "label: 7, acc: 86.30\n",
      "label: 8, acc: 91.80\n",
      "label: 9, acc: 90.30\n",
      "测试集Acc：0.8223\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_1/Coteachingplus/cifar-resnet18_worker_restore.pth\n",
      "test_acc: 80.62\n",
      "label: 0, acc: 84.20\n",
      "label: 1, acc: 90.50\n",
      "label: 2, acc: 69.70\n",
      "label: 3, acc: 65.90\n",
      "label: 4, acc: 77.50\n",
      "label: 5, acc: 67.90\n",
      "label: 6, acc: 86.40\n",
      "label: 7, acc: 84.80\n",
      "label: 8, acc: 90.00\n",
      "label: 9, acc: 89.30\n",
      "测试集Acc：0.8062\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_2/Coteachingplus/cifar-resnet18_worker_restore.pth\n",
      "test_acc: 78.40\n",
      "label: 0, acc: 83.90\n",
      "label: 1, acc: 91.20\n",
      "label: 2, acc: 66.20\n",
      "label: 3, acc: 60.40\n",
      "label: 4, acc: 76.90\n",
      "label: 5, acc: 69.10\n",
      "label: 6, acc: 83.70\n",
      "label: 7, acc: 80.50\n",
      "label: 8, acc: 85.00\n",
      "label: 9, acc: 87.10\n",
      "测试集Acc：0.784\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_3/Coteachingplus/cifar-resnet18_worker_restore.pth\n",
      "test_acc: 76.53\n",
      "label: 0, acc: 83.90\n",
      "label: 1, acc: 87.90\n",
      "label: 2, acc: 68.70\n",
      "label: 3, acc: 56.60\n",
      "label: 4, acc: 78.70\n",
      "label: 5, acc: 60.90\n",
      "label: 6, acc: 82.70\n",
      "label: 7, acc: 75.00\n",
      "label: 8, acc: 87.10\n",
      "label: 9, acc: 83.80\n",
      "测试集Acc：0.7653\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_0/JoCoR/cifar-resnet18_worker_restore.pth\n",
      "test_acc: 82.23\n",
      "label: 0, acc: 87.30\n",
      "label: 1, acc: 95.40\n",
      "label: 2, acc: 72.80\n",
      "label: 3, acc: 59.30\n",
      "label: 4, acc: 76.50\n",
      "label: 5, acc: 74.50\n",
      "label: 6, acc: 88.10\n",
      "label: 7, acc: 86.30\n",
      "label: 8, acc: 91.80\n",
      "label: 9, acc: 90.30\n",
      "测试集Acc：0.8223\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_1/JoCoR/cifar-resnet18_worker_restore.pth\n",
      "test_acc: 81.47\n",
      "label: 0, acc: 83.50\n",
      "label: 1, acc: 91.90\n",
      "label: 2, acc: 76.00\n",
      "label: 3, acc: 61.30\n",
      "label: 4, acc: 79.90\n",
      "label: 5, acc: 71.70\n",
      "label: 6, acc: 86.50\n",
      "label: 7, acc: 84.40\n",
      "label: 8, acc: 90.10\n",
      "label: 9, acc: 89.40\n",
      "测试集Acc：0.8147\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_2/JoCoR/cifar-resnet18_worker_restore.pth\n",
      "test_acc: 79.89\n",
      "label: 0, acc: 84.80\n",
      "label: 1, acc: 91.00\n",
      "label: 2, acc: 71.20\n",
      "label: 3, acc: 62.60\n",
      "label: 4, acc: 79.40\n",
      "label: 5, acc: 69.60\n",
      "label: 6, acc: 83.20\n",
      "label: 7, acc: 83.90\n",
      "label: 8, acc: 86.50\n",
      "label: 9, acc: 86.70\n",
      "测试集Acc：0.7989\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_3/JoCoR/cifar-resnet18_worker_restore.pth\n",
      "test_acc: 77.52\n",
      "label: 0, acc: 84.50\n",
      "label: 1, acc: 86.50\n",
      "label: 2, acc: 74.10\n",
      "label: 3, acc: 50.50\n",
      "label: 4, acc: 79.00\n",
      "label: 5, acc: 63.90\n",
      "label: 6, acc: 85.70\n",
      "label: 7, acc: 76.40\n",
      "label: 8, acc: 88.40\n",
      "label: 9, acc: 86.20\n",
      "测试集Acc：0.7752\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_0/contra/cifar-resnet18_worker_restore.pth\n",
      "test_acc: 82.23\n",
      "label: 0, acc: 87.30\n",
      "label: 1, acc: 95.40\n",
      "label: 2, acc: 72.80\n",
      "label: 3, acc: 59.30\n",
      "label: 4, acc: 76.50\n",
      "label: 5, acc: 74.50\n",
      "label: 6, acc: 88.10\n",
      "label: 7, acc: 86.30\n",
      "label: 8, acc: 91.80\n",
      "label: 9, acc: 90.30\n",
      "测试集Acc：0.8223\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_1/contra/cifar-resnet18_worker_tta.pth\n",
      "test_acc: 81.69\n",
      "label: 0, acc: 91.50\n",
      "label: 1, acc: 96.80\n",
      "label: 2, acc: 73.20\n",
      "label: 3, acc: 59.00\n",
      "label: 4, acc: 61.60\n",
      "label: 5, acc: 74.00\n",
      "label: 6, acc: 92.00\n",
      "label: 7, acc: 91.50\n",
      "label: 8, acc: 88.10\n",
      "label: 9, acc: 89.20\n",
      "测试集Acc：0.8169\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_2/contra/cifar-resnet18_worker_tta.pth\n",
      "test_acc: 80.73\n",
      "label: 0, acc: 89.00\n",
      "label: 1, acc: 96.80\n",
      "label: 2, acc: 73.30\n",
      "label: 3, acc: 54.80\n",
      "label: 4, acc: 55.90\n",
      "label: 5, acc: 74.30\n",
      "label: 6, acc: 93.20\n",
      "label: 7, acc: 91.50\n",
      "label: 8, acc: 90.00\n",
      "label: 9, acc: 88.50\n",
      "测试集Acc：0.8073\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/cifar-10/nr_0.2_nt_symmetric_balanced/step_3/contra/cifar-resnet18_worker_tta.pth\n",
      "test_acc: 80.43\n",
      "label: 0, acc: 89.50\n",
      "label: 1, acc: 97.10\n",
      "label: 2, acc: 67.80\n",
      "label: 3, acc: 57.70\n",
      "label: 4, acc: 56.80\n",
      "label: 5, acc: 74.80\n",
      "label: 6, acc: 92.10\n",
      "label: 7, acc: 91.60\n",
      "label: 8, acc: 90.70\n",
      "label: 9, acc: 86.20\n",
      "测试集Acc：0.8043\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'cifar-10'\n",
    "model_name = 'cifar-resnet18'\n",
    "noise_type='symmetric'\n",
    "\n",
    "# ------------------------------------------------------------- #\n",
    "\"\"\"\n",
    "实验区：手动指定要评估的组\n",
    "methods: ['raw', 'coteaching', 'coteaching_plus', 'jocor', 'cotta', 'plf', 'contra']\n",
    "steps: [0, 1, 2, 3]\n",
    "\"\"\"\n",
    "# methods = ['contra']\n",
    "# methods = ['JoCoR']\n",
    "# methods = ['raw']\n",
    "# methods = ['cotta']\n",
    "# methods = ['raw', 'coteaching', 'coteaching_plus', 'jocor', 'cotta', 'plf', 'contra']\n",
    "# methods = ['raw', 'replay', 'Coteaching', 'Coteachingplus', 'JoCoR', 'cotta', 'plf', 'contra']\n",
    "methods = ['raw', 'replay', 'Coteaching', 'Coteachingplus', 'JoCoR', 'contra']\n",
    "steps = [i for i in range(4)]\n",
    "# steps = [0]\n",
    "# ------------------------------------------------------------- #\n",
    "\"\"\"\n",
    "指定结果存储的路径\n",
    "\"\"\"\n",
    "results_dir = './results_main'\n",
    "# ------------------------------------------------------------- #\n",
    "\n",
    "eva_test_acc(dataset_name, model_name, noise_type, \n",
    "             methods=methods, steps=steps,\n",
    "             results_dir=results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pet-37 分类\n",
    "> 评估测试集acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'pet-37'\n",
    "model_name = 'wideresnet50'\n",
    "noise_type='symmetric'\n",
    "\n",
    "# ------------------------------------------------------------- #\n",
    "\"\"\"\n",
    "实验区：手动指定要评估的组\n",
    "methods: ['raw', 'coteaching', 'coteaching_plus', 'jocor', 'cotta', 'plf', 'contra']\n",
    "steps: [0, 1, 2, 3]\n",
    "\"\"\"\n",
    "# methods = ['contra']\n",
    "# methods = ['raw']\n",
    "# methods = ['cotta']\n",
    "# methods = ['plf']\n",
    "# methods = ['raw', 'replay', 'Coteaching', 'Coteachingplus', 'JoCoR', 'cotta', 'plf', 'contra']\n",
    "# methods = ['raw', 'coteaching', 'coteaching_plus', 'jocor', 'contra']\n",
    "steps = [i for i in range(4)]\n",
    "# steps = [0]\n",
    "# ------------------------------------------------------------- #\n",
    "\"\"\"\n",
    "指定结果存储的路径\n",
    "\"\"\"\n",
    "results_dir = './results_main'\n",
    "# ------------------------------------------------------------- #\n",
    "\n",
    "eva_test_acc(dataset_name, model_name, noise_type, \n",
    "             methods=methods, steps=steps, results_dir=results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cifar-100 检索\n",
    "> 计算mAP（test_acc仅供参考）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'cifar-100'\n",
    "model_name = 'cifar-wideresnet40'\n",
    "noise_type= 'asymmetric'\n",
    "\n",
    "# ------------------------------------------------------------- #\n",
    "\"\"\"\n",
    "实验区：手动指定要评估的组\n",
    "methods: ['raw', 'coteaching', 'coteaching_plus', 'jocor', 'cotta', 'plf', 'contra']\n",
    "steps: [0, 1, 2, 3]\n",
    "\"\"\"\n",
    "# methods = ['contra']\n",
    "# methods = ['raw']\n",
    "# methods = ['cotta']\n",
    "# methods = ['cotta', 'plf']\n",
    "# methods = ['raw', 'coteaching', 'coteaching_plus', 'jocor', 'cotta', 'plf', 'contra']\n",
    "# methods = ['raw', 'replay', 'Coteaching', 'Coteachingplus', 'JoCoR', 'cotta', 'plf', 'contra']\n",
    "# methods = ['raw', 'replay', 'Coteaching', 'JoCoR', 'cotta', 'contra']\n",
    "\n",
    "methods = ['cotta', 'plf']\n",
    "# methods = ['replay', 'contra']\n",
    "steps = [i for i in range(4)]\n",
    "# steps = [3]\n",
    "# ------------------------------------------------------------- #\n",
    "\"\"\"\n",
    "指定结果存储的路径\n",
    "\"\"\"\n",
    "results_dir = './results_main'\n",
    "# ------------------------------------------------------------- #\n",
    "eva_test_acc(dataset_name, model_name, noise_type, \n",
    "             methods=methods, steps=steps, results_dir=results_dir)\n",
    "\n",
    "# eva_map(dataset_name, model_name, noise_type, \n",
    "#         methods=methods, steps=steps, results_dir=results_dir,\n",
    "#         top_k=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 图片检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'cifar-100'\n",
    "model_name = 'cifar-wideresnet40'\n",
    "noise_type= 'asymmetric'\n",
    "\n",
    "# ------------------------------------------------------------- #\n",
    "\"\"\"\n",
    "实验区：手动指定要评估的组\n",
    "methods: ['raw', 'coteaching', 'coteaching_plus', 'jocor', 'cotta', 'plf', 'contra']\n",
    "steps: [0, 1, 2, 3]\n",
    "\"\"\"\n",
    "\n",
    "# methods = ['raw']\n",
    "# methods = ['Coteaching']\n",
    "# methods = ['JoCoR']\n",
    "methods = [\"contra\"]\n",
    "\n",
    "# methods = ['raw', 'coteaching', 'coteaching_plus', 'jocor', 'cotta', 'plf', 'contra']\n",
    "# methods = ['raw', 'coteaching', 'coteaching_plus', 'jocor', 'contra']\n",
    "# steps = [i for i in range(4)]\n",
    "steps = [3]\n",
    "# ------------------------------------------------------------- #\n",
    "\n",
    "\"\"\"\n",
    "检索用参数\n",
    "\"\"\"\n",
    "top_k = 5\n",
    "query_indices = [333]\n",
    "# ------------------------------------------------------------- #\n",
    "# eva_retrieval(dataset_name, model_name, noise_type,\n",
    "#               methods=methods, steps=steps, \n",
    "#               top_k=top_k, query_indices=query_indices)\n",
    "\n",
    "# 已经测试过一次的时候，用这个：\n",
    "eva_retrieval(dataset_name, model_name, noise_type,\n",
    "              methods=methods, steps=steps, \n",
    "              top_k=top_k, query_indices=query_indices,\n",
    "              use_cache=True)\n",
    "\n",
    "# 322-flatfish\n",
    "# 333， 334\n",
    "\n",
    "\n",
    "# for i in range(331, 340):\n",
    "#     query_indices = [i]\n",
    "#     eva_retrieval(dataset_name, model_name, noise_type,\n",
    "#                 methods=methods, steps=steps, \n",
    "#                 top_k=top_k, query_indices=query_indices,\n",
    "#                 use_cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pet-37 检索\n",
    "> 计算mAP（test_acc仅供参考）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目前测试的数据集：pet-37, case模式：nr_0.2_nt_asymmetric_balanced\n",
      "Loading /nvme/sunzekun/Projects/tta-mr/data/pet-37/gen/nr_0.2_nt_asymmetric_balanced/test_data.npy\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/pet-37/nr_0.2_nt_asymmetric_balanced/step_1/cotta/wideresnet50_worker_tta.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_252992/4186978523.py:83: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_repair_save_path)\n",
      "/nvme/sunzekun/Projects/tta-mr/baseline_code/cotta-main/cifar/cotta.py:203: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(load_model_path)\n",
      "100%|██████████| 58/58 [04:21<00:00,  4.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集Acc：0.2221313709457618\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/pet-37/nr_0.2_nt_asymmetric_balanced/step_2/cotta/wideresnet50_worker_tta.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [03:53<00:00,  4.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集Acc：0.22267647860452439\n",
      "Evaluating /nvme/sunzekun/Projects/tta-mr/ckpt/pet-37/nr_0.2_nt_asymmetric_balanced/step_3/cotta/wideresnet50_worker_tta.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [03:50<00:00,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集Acc：0.22567457072771874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"pet-37\"\n",
    "model_name = \"wideresnet50\"\n",
    "noise_type = \"asymmetric\"\n",
    "\n",
    "# ------------------------------------------------------------- #\n",
    "\"\"\"\n",
    "实验区：手动指定要评估的组\n",
    "methods: ['raw', 'coteaching', 'coteaching_plus', 'jocor', 'cotta', 'plf', 'contra']\n",
    "steps: [0, 1, 2, 3]\n",
    "\"\"\"\n",
    "# methods = [\"contra\"]\n",
    "# methods = ['raw']\n",
    "# methods = ['raw', 'replay', 'Coteaching', 'Coteachingplus', 'JoCoR', 'cotta', 'plf', 'contra']\n",
    "# methods = ['raw', 'replay', 'Coteaching', 'Coteachingplus', 'JoCoR', 'contra']\n",
    "# methods = ['raw', 'coteaching', 'coteaching_plus', 'jocor', 'contra']\n",
    "# methods = ['cotta', 'plf']\n",
    "methods = ['cotta']\n",
    "# steps = [i for i in range(4)]\n",
    "# steps = [0]\n",
    "steps = [1, 2, 3]\n",
    "# ------------------------------------------------------------- #\n",
    "\"\"\"\n",
    "指定结果存储的路径\n",
    "\"\"\"\n",
    "results_dir = \"./results_main\"\n",
    "# ------------------------------------------------------------- #\n",
    "eva_test_acc(dataset_name, model_name, noise_type,\n",
    "             methods=methods, steps=steps, results_dir=results_dir)\n",
    "# eva_map(\n",
    "#     dataset_name,\n",
    "#     model_name,\n",
    "#     noise_type,\n",
    "#     methods=methods,\n",
    "#     steps=steps,\n",
    "#     results_dir=results_dir,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 图片检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"pet-37\"\n",
    "model_name = \"wideresnet50\"\n",
    "noise_type = \"asymmetric\"\n",
    "\n",
    "# ------------------------------------------------------------- #\n",
    "\"\"\"\n",
    "实验区：手动指定要评估的组\n",
    "methods: ['raw', 'coteaching', 'coteaching_plus', 'jocor', 'cotta', 'plf', 'contra']\n",
    "steps: [0, 1, 2, 3]\n",
    "\"\"\"\n",
    "methods = [\"contra\"]\n",
    "# methods = ['raw']\n",
    "# methods = ['Coteaching']\n",
    "# methods = ['JoCoR']\n",
    "# methods = ['cotta']\n",
    "# methods = ['raw', 'coteaching', 'coteaching_plus', 'jocor', 'cotta', 'plf', 'contra']\n",
    "# methods = ['raw', 'coteaching', 'coteaching_plus', 'jocor', 'contra']\n",
    "# steps = [i for i in range(4)]\n",
    "steps = [3]\n",
    "# ------------------------------------------------------------- #\n",
    "\"\"\"\n",
    "检索用参数\n",
    "\"\"\"\n",
    "top_k = 5\n",
    "# query_indices = [3102] # Shiba Inu\n",
    "# query_indices = [3102]\n",
    "\n",
    "\n",
    "# query_indices = [2321] # Perisian\n",
    "# query_indices = [444] # 币哥\n",
    "# query_indices = [303] # \n",
    "\n",
    "query_indices = [321]\n",
    "\n",
    "# ------------------------------------------------------------- #\n",
    "# eva_retrieval(dataset_name, model_name, noise_type,\n",
    "#               methods=methods, steps=steps, \n",
    "#               top_k=top_k, query_indices=query_indices)\n",
    "\n",
    "# 已经计算过一次分数的时候，用这个：\n",
    "eva_retrieval(dataset_name, model_name, noise_type,\n",
    "              methods=methods, steps=steps, \n",
    "              top_k=top_k, query_indices=query_indices,\n",
    "              use_cache=True)\n",
    "\n",
    "\n",
    "# # 303、304\n",
    "# for i in range(321, 330):\n",
    "#     query_indices = [i]\n",
    "#     eva_retrieval(dataset_name, model_name, noise_type,\n",
    "#                 methods=methods, steps=steps, \n",
    "#                 top_k=top_k, query_indices=query_indices,\n",
    "#                 use_cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 敏感度实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'pet-37'\n",
    "model_name = 'wideresnet50'\n",
    "\n",
    "# ------------------------------------------------------------- #\n",
    "\"\"\"\n",
    "实验区：手动指定要评估的组\n",
    "methods: ['raw', 'coteaching', 'contra']\n",
    "steps: [0, 1, 2, 3]\n",
    "\"\"\"\n",
    "# methods = ['raw', 'Coteaching', 'contra']\n",
    "methods = ['raw', 'replay', 'Coteaching', 'contra']\n",
    "steps = [i for i in range(4)]\n",
    "# ------------------------------------------------------------- #\n",
    "\"\"\"\n",
    "指定结果存储的路径\n",
    "\"\"\"\n",
    "results_dir = './results_sensitivity'\n",
    "# ------------------------------------------------------------- #\n",
    "\n",
    "\n",
    "\n",
    "for noise_ratio in [0.1, 0.3, 0.5]:\n",
    "        # 分类任务评估\n",
    "        results_dir = f'./results_sensitivity/nr_{noise_ratio}'\n",
    "        eva_test_acc(dataset_name, model_name,\n",
    "                     noise_ratio=noise_ratio, \n",
    "                     noise_type='symmetric',                     \n",
    "                     methods=methods, steps=steps, results_dir=results_dir)\n",
    "\n",
    "        # 检测任务评估\n",
    "        eva_map(dataset_name, model_name,\n",
    "                noise_ratio=noise_ratio,\n",
    "                noise_type='asymmetric',\n",
    "                methods=methods, steps=steps, results_dir=results_dir)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 结果绘图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(results_file_path, type='cls'):\n",
    "    data = pd.read_csv(results_file_path, index_col=0)\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    # fig, ax = plt.subplots(figsize=(4, 5))\n",
    "    ax = plt.gca()\n",
    "    ax.set_facecolor('#EAEAF2')\n",
    "    # color_list = ['#696969', '#2799B2', '#4DA764']\n",
    "\n",
    "    lw = 2\n",
    "    ms = 10 # marker_size\n",
    "    tfs = 12 # tick font size\n",
    "    \n",
    "    plt.plot(data.columns, data.loc['raw'], label='raw', color='#696969', linestyle='--', linewidth=lw, marker='o', markersize=ms) # Grey dashed line\n",
    "    plt.plot(data.columns, data.loc['Coteaching'], label='coteaching', color='#2799B2', linewidth=lw, marker='^', markersize=ms) # Blue solid line\n",
    "    plt.plot(data.columns, data.loc['contra'], label='contra', color='#4DA764', linewidth=lw, marker='s', markersize=ms) # Green solid line\n",
    "    plt.plot(data.columns, data.loc['replay'], label='raw', color='#37D4AD', linewidth=lw, marker='p', markersize=ms) # Green solid line\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # for i, label in enumerate(data.index):\n",
    "    #     plt.plot(data.columns, data.loc[label], label=label, color=color_list[i])\n",
    "    \n",
    "    \n",
    "    \n",
    "    if type == 'cls':\n",
    "\n",
    "        # plt.yticks(np.arange(0.75, 0.96, 0.05), [''] * len(np.arange(0.75, 0.96, 0.05)), fontsize=tfs, rotation=90, va='center', fontfamily=\"serif\", weight=\"bold\")\n",
    "        # plt.yticks(np.arange(0.25, 0.95, 0.2), fontsize=tfs, rotation=90, va='center', fontfamily=\"serif\", weight=\"bold\")\n",
    "        plt.yticks(np.arange(0.80, 0.96, 0.04), fontsize=tfs, rotation=90, va='center', fontfamily=\"serif\", weight=\"bold\")\n",
    "    else:\n",
    "        plt.yticks(np.arange(0.88, 0.95, 0.02), fontsize=tfs, rotation=90, va='center', fontfamily=\"serif\", weight=\"bold\")\n",
    "\n",
    "    plt.xticks(fontsize=tfs, fontfamily=\"serif\", weight=\"bold\")\n",
    "    # plt.title('Performance Comparison')\n",
    "    # plt.xlabel('Epoch')\n",
    "    # plt.ylabel('Accuracy')\n",
    "    # plt.legend()\n",
    "    plt.grid(True, color='white')\n",
    "    \n",
    "    # plt.subplots_adjust(top = 1, bottom = 0, right = 1, left = 0)\n",
    "\n",
    "    # plt.subplots_adjust(left=0.05, right=0.95, top=0.95, bottom=0.05)\n",
    "    # plt.axis('off')\n",
    "    # plt.margins(0,0)\n",
    "    # plt.tight_layout()\n",
    "\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_file_path = f'./results_sensitivity/nr_0.1/pet-37_cls.csv'\n",
    "# visualize(results_file_path)\n",
    "for noise_ratio in [0.1, 0.3, 0.5]:\n",
    "    results_file_path = f'./results_sensitivity/nr_{noise_ratio}/pet-37_cls.csv'\n",
    "    print(f'Visualizing {results_file_path}')\n",
    "    visualize(results_file_path)\n",
    "\n",
    "\n",
    "for noise_ratio in [0.1, 0.3, 0.5]:\n",
    "    results_file_path = f'./results_sensitivity/nr_{noise_ratio}/pet-37_rtv.csv'\n",
    "    print(f'Visualizing {results_file_path}')\n",
    "    visualize(results_file_path, type='rtv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation Study 评估\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 分类任务（Pet-37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'pet-37'\n",
    "model_name = 'wideresnet50'\n",
    "noise_type='symmetric'\n",
    "\n",
    "\"\"\"\n",
    "注意！！！\n",
    "这个函数使用之前记得去get_suffix里边进行相应代码的切换\n",
    "因为它的读取策略和普通的实验不同。\n",
    "\"\"\"\n",
    "# ------------------------------------------------------------- #\n",
    "\"\"\"\n",
    "实验区：手动指定要评估的组\n",
    "methods: ['contra_repair_only', 'contra_tta_only']\n",
    "steps: [0, 1, 2, 3]\n",
    "\"\"\"\n",
    "\n",
    "# methods = ['contra', 'contra_tta_only'] # 其中contra会返回worker_restore的结果，这个是restore-only，而后面那个是tta_only\n",
    "methods = ['contra_tta_only']\n",
    "# methods = ['contra']\n",
    "steps = [i for i in range(4)]\n",
    "# ------------------------------------------------------------- #\n",
    "\"\"\"\n",
    "指定结果存储的路径\n",
    "\"\"\"\n",
    "results_dir = './results_ablation'\n",
    "# ------------------------------------------------------------- #\n",
    "\n",
    "eva_test_acc(dataset_name, model_name, noise_type, \n",
    "             methods=methods, steps=steps, results_dir=results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 检索任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"pet-37\"\n",
    "model_name = \"wideresnet50\"\n",
    "noise_type = \"asymmetric\"\n",
    "\n",
    "# ------------------------------------------------------------- #\n",
    "\"\"\"\n",
    "实验区：手动指定要评估的组\n",
    "methods: ['raw', 'coteaching', 'coteaching_plus', 'jocor', 'cotta', 'plf', 'contra']\n",
    "steps: [0, 1, 2, 3]\n",
    "\"\"\"\n",
    "\n",
    "# methods = ['contra', 'contra_tta_only'] # 其中contra会返回worker_restore的结果，这个是restore-only，而后面那个是tta_only\n",
    "methods = ['contra_tta_only']\n",
    "steps = [i for i in range(4)]\n",
    "# steps = [0]\n",
    "# ------------------------------------------------------------- #\n",
    "\"\"\"\n",
    "指定结果存储的路径\n",
    "\"\"\"\n",
    "results_dir = \"./results_ablation\"\n",
    "# ------------------------------------------------------------- #\n",
    "eva_test_acc(dataset_name, model_name, noise_type,\n",
    "             methods=methods, steps=steps, results_dir=results_dir)\n",
    "# eva_map(\n",
    "#     dataset_name,\n",
    "#     model_name,\n",
    "#     noise_type,\n",
    "#     methods=methods,\n",
    "#     steps=steps,\n",
    "#     results_dir=results_dir,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSNE\n",
    "> 主实验-cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from MulticoreTSNE import MulticoreTSNE as TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def load_model_and_data(\n",
    "    dataset_name, model_name, step, method, noise_type, noise_ratio\n",
    "):\n",
    "    \"\"\"\n",
    "    加载模型及其对应的测试数据集，并返回模型和数据加载器\n",
    "    \"\"\"\n",
    "    # 构建数据集和模型的case\n",
    "    case = settings.get_case(\n",
    "        noise_ratio=noise_ratio, noise_type=noise_type, balanced=True\n",
    "    )\n",
    "\n",
    "    num_classes = settings.num_classes_dict[dataset_name]\n",
    "\n",
    "    # 加载测试数据集\n",
    "    batch_size = 64\n",
    "    test_data, test_labels, test_dataloader = get_dataset_loader(\n",
    "        dataset_name,\n",
    "        \"test\",\n",
    "        case,\n",
    "        None,\n",
    "        None,\n",
    "        None,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "    )\n",
    "\n",
    "    # 加载模型\n",
    "    model_suffix = get_suffix(method, step)\n",
    "    model = load_custom_model(model_name, num_classes, load_pretrained=False)\n",
    "    model = ClassifierWrapper(model, num_classes)\n",
    "\n",
    "    # 生成正确的模型路径，增加 \"cifar-\" 前缀\n",
    "    model_path = settings.get_ckpt_path(\n",
    "        dataset_name,\n",
    "        case,\n",
    "        model_name,\n",
    "        model_suffix=model_suffix,  # 增加 'cifar-' 前缀\n",
    "        step=step,\n",
    "        unique_name=method,\n",
    "    )\n",
    "\n",
    "    # 加载模型权重\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint, strict=False)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    return model, test_dataloader, test_labels\n",
    "\n",
    "\n",
    "# 获取嵌入数据\n",
    "def get_embeddings(model, dataloader, layer_name=\"feature_model\"):\n",
    "    \"\"\"\n",
    "    提取模型的嵌入层输出\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "\n",
    "    # 注册hook来捕获指定层的输出\n",
    "    def hook_fn(module, input, output):\n",
    "        embeddings.append(output.detach().cpu().numpy())\n",
    "\n",
    "    handle = model._modules.get(layer_name).register_forward_hook(hook_fn)\n",
    "\n",
    "    # 进行前向传播以获取嵌入\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            model(inputs)\n",
    "\n",
    "    handle.remove()\n",
    "\n",
    "    # 拼接所有批次的嵌入\n",
    "    embeddings = np.concatenate(embeddings, axis=0)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def get_cache_path_tsne(dataset_name, case, step, unique_name):\n",
    "    root_dir = os.path.abspath(os.path.join(os.getcwd(), os.path.pardir))\n",
    "    \n",
    "    path = os.path.join(root_dir, \"result_analysis/results_tsne/\", dataset_name, case, f\"step_{step}\", unique_name)\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    return os.path.join(path, f\"tsne.npz\")\n",
    "\n",
    "\n",
    "\n",
    "# 绘制 t-SNE\n",
    "def get_tsne(dataset_name, model_name, step, method, noise_type, noise_ratio):\n",
    "    \"\"\"\n",
    "    绘制多个step的t-SNE结果，展示Contra方法在不同步骤的嵌入变化\n",
    "    \"\"\"\n",
    "    # all_embeddings = []\n",
    "    # all_labels = []\n",
    "\n",
    "    # 遍历不同的steps和方法，获取模型嵌入\n",
    "    # for step in steps:\n",
    "    #     model, dataloader = load_model_and_data(\n",
    "    #         dataset_name, model_name, step, methods, noise_type, noise_ratio\n",
    "    #     )\n",
    "    #     # print(model)\n",
    "    #     # return\n",
    "    #     embeddings = get_embeddings(model, dataloader)\n",
    "    #     all_embeddings.append(embeddings)\n",
    "    #     all_labels += [f\"step_{step}\"] * len(embeddings)\n",
    "\n",
    "    model, dataloader, test_labels = load_model_and_data(\n",
    "            dataset_name, model_name, step, method, noise_type, noise_ratio\n",
    "    )    \n",
    "    embeddings = get_embeddings(model, dataloader)\n",
    "\n",
    "    # 将所有嵌入拼接\n",
    "    # embeddings_concat = np.concatenate(all_embeddings, axis=0)\n",
    "    # print(embeddings_concat.shape)\n",
    "#   return\n",
    "    print(embeddings.shape)\n",
    "\n",
    "    # PCA 降维\n",
    "    pca = PCA(n_components=50)\n",
    "    pca_result = pca.fit_transform(embeddings)\n",
    "\n",
    "    # t-SNE降维\n",
    "    tsne = TSNE(n_components=2, n_jobs=8, random_state=42)\n",
    "    tsne_result = tsne.fit_transform(pca_result)\n",
    "\n",
    "    return tsne_result\n",
    "\n",
    "    # 绘制结果\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    unique_labels = list(set(test_labels))\n",
    "    colors = plt.cm.get_cmap(\"tab10\", len(unique_labels))\n",
    "\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        indices = [j for j, lbl in enumerate(test_labels) if lbl == label]\n",
    "        plt.scatter(\n",
    "            tsne_result[indices, 0],\n",
    "            tsne_result[indices, 1],\n",
    "            label=label,\n",
    "            color=colors(i),\n",
    "            s=50,\n",
    "            alpha=0.6,\n",
    "        )\n",
    "\n",
    "    plt.title(f\"t-SNE of Contra on {dataset_name}\", fontsize=20)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_tsne(methods, steps, dataset_name, model_name, noise_type, noise_ratio, use_cache=False):\n",
    "    case = settings.get_case(noise_ratio=noise_ratio, noise_type=noise_type, balanced=True)\n",
    "    plt.style.use(\"default\")\n",
    "    fig, axs = plt.subplots(len(methods), len(steps), figsize=(9, 9))\n",
    "    tsne_results_all = {}\n",
    "    for method in methods:\n",
    "        tsne_resuls_step = {}\n",
    "        for step in steps:\n",
    "            cache_path = get_cache_path_tsne(dataset_name=dataset_name, case=case, step=step, unique_name=method)\n",
    "            # print(cache_path)\n",
    "            if use_cache:\n",
    "                try:\n",
    "                    data = np.load(cache_path)\n",
    "                except:\n",
    "                    print(\"未读取到对应的tsne。首次评估某个模型时，请先使用use_cache=False\")\n",
    "                    return\n",
    "\n",
    "                print(f'读取缓存的tsne记录：{cache_path}')\n",
    "                tsne = data['tsne']\n",
    "                tsne_resuls_step[step] = tsne\n",
    "            else:\n",
    "                tsne = get_tsne(dataset_name, model_name, step, method, noise_type, noise_ratio)\n",
    "                np.savez(cache_path, tsne=tsne)\n",
    "                print(f'缓存tsne结果至：{cache_path}')\n",
    "\n",
    "        tsne_results_all[method] = tsne_resuls_step\n",
    "    \n",
    "    print(tsne_results_all[methods[0]][steps[0]])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取缓存的tsne记录：/nvme/sunzekun/Projects/tta-mr/result_analysis/results_tsne/cifar-10/nr_0.2_nt_symmetric_balanced/step_1/contra/cache.npz\n",
      "[[  2.67909225  15.93961709]\n",
      " [-19.13999458 -16.1369148 ]\n",
      " [-28.32519413  -3.51840435]\n",
      " ...\n",
      " [  2.45110918  36.69433014]\n",
      " [-31.79193771   2.77674246]\n",
      " [ 12.62126218 -33.23843186]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x900 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 执行 t-SNE 可视化\n",
    "# steps = [0, 1, 2, 3]\n",
    "# steps = [1, 2, 3]\n",
    "# step = [1, 2, 3]\n",
    "\n",
    "steps = [1, 2, 3]\n",
    "# methods = ['contra']\n",
    "# methods = ['raw', 'replay', 'Coteaching', 'Coteachingplus', 'JoCoR', 'contra']\n",
    "methods = ['raw', 'replay', 'Coteaching', 'cotta', 'contra']\n",
    "# method = 'contra'\n",
    "dataset_name = 'cifar-10'\n",
    "model_name = 'cifar-resnet18'\n",
    "noise_type = 'symmetric'\n",
    "noise_ratio = 0.2\n",
    "\n",
    "\n",
    "# get_tsne(dataset_name, model_name, step, method, noise_type, noise_ratio)\n",
    "plot_tsne(methods, steps, dataset_name, model_name, noise_type, noise_ratio, use_cache=False)\n",
    "# plot_tsne(methods, steps, dataset_name, model_name, noise_type, noise_ratio, use_cache=True)\n",
    "\n",
    "# plot_tsne_contra(\n",
    "#     dataset_name=dataset_name,\n",
    "#     model_name=model_name,\n",
    "#     steps=step,\n",
    "#     methods=methods,\n",
    "#     noise_type=noise_type,\n",
    "#     noise_ratio=noise_ratio\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apbench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
